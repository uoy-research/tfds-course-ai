{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0846524",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Matthew Care\"\n",
    "__version__ = \"0.0.7\"\n",
    "__date__ = \"2026-01-27\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47043492",
   "metadata": {},
   "source": [
    "# Coding Exercise 2 - Pipelines, Metrics and Hyperparameter Tuning on the Adult Census Income Dataset\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "1. How to build production-ready ML pipelines that prevent data leakage\n",
    "2. How to handle imbalanced datasets using resampling techniques\n",
    "3. How to select and interpret appropriate evaluation metrics for imbalanced classification\n",
    "4. How to incorporate misclassification costs into model optimisation\n",
    "5. How to tune hyperparameters systematically using Optuna\n",
    "6. How to optimise decision thresholds for different objectives\n",
    "7. How to combine multiple models using ensemble methods\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds on Coding Exercise 1 and implements a more **robust ML workflow** using:\n",
    "- [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to avoid data leakage,\n",
    "- **Imbalanced-learn** sampling strategies (SMOTE, ADASYN, undersampling, hybrid methods) to handle class imbalance,\n",
    "- Richer **classification metrics** (accuracy, precision, recall, F1, MCC, ROC/PR AUC, balanced accuracy, specificity, cost/benefit analysis),\n",
    "- **Hyperparameter tuning** with [Optuna](https://optuna.org/) using flexible objective functions,\n",
    "- **Threshold optimisation** to maximise performance for specific objectives,\n",
    "- **Ensemble methods** (voting and stacking) to combine multiple models,\n",
    "- **Model interpretability** tools (feature importance, learning curves, calibration plots, partial dependence).\n",
    "\n",
    "For EDA (exploring feature distributions, correlations, class imbalance), **see Coding Exercise 1** - here we focus on pipelines, metrics and tuning.\n",
    "\n",
    "## 0. Setup\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the configuration parameters that control the pipeline\n",
    "- Learn how to customise the workflow for different objectives\n",
    "- Ensure reproducibility through proper random state management\n",
    "\n",
    "### Configuration Guide\n",
    "\n",
    "This notebook provides extensive configuration options. The key parameters you can modify are:\n",
    "\n",
    "**Execution Mode:**\n",
    "- `QUICK_MODE`: Set to `True` for faster runs during development/testing, `False` for full evaluation\n",
    "\n",
    "**Model Selection:**\n",
    "- `MODEL_NAMES`: List of models to evaluate (from `ALL_MODELS`)\n",
    "- `TUNED_MODELS`: Subset of models to hyperparameter tune with Optuna\n",
    "\n",
    "**Optimisation Objective:**\n",
    "- `TUNING_OBJECTIVE`: Choose from `\"mcc\"`, `\"f1\"`, `\"balanced_accuracy\"`, or `\"cost_benefit\"`\n",
    "- `COST_FP`, `COST_FN`, `BENEFIT_TP`, `BENEFIT_TN`: Configure misclassification costs (used when `TUNING_OBJECTIVE = \"cost_benefit\"`)\n",
    "\n",
    "**Threshold Optimisation:**\n",
    "- `USE_OPTIMAL_THRESHOLD`: Whether to find and apply optimal decision threshold\n",
    "- `THRESHOLD_METRIC`: Metric to optimise threshold for (`\"auto\"` uses `TUNING_OBJECTIVE`)\n",
    "\n",
    "**Ensemble Configuration:**\n",
    "- `ENSEMBLE_METHOD`: Choose `\"voting\"` or `\"stacking\"`\n",
    "- `TOP_N_MODELS_FOR_ENSEMBLE`: Number of best models to combine\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "- `SAMPLING_METHODS`: Resampling strategies to evaluate (SMOTE, ADASYN, etc.)\n",
    "\n",
    "**Cross-Validation:**\n",
    "- `N_SPLITS`: Number of CV folds\n",
    "- `N_REPEATS_CV`: Number of CV repetitions for final evaluation\n",
    "\n",
    "For hyperparameter search space customisation, see Section 9 (`suggest_params_for_model`).\n",
    "\n",
    "**Reproducibility:** All random operations use `RANDOM_STATE` to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352530de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and global parameters\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.5  # 50% train / 50% test, as in Coding Exercise 1\n",
    "\n",
    "# Quick mode for faster development and testing\n",
    "QUICK_MODE = True  # Set to True for faster runs with reduced trials/splits\n",
    "\n",
    "# Cross-validation configuration\n",
    "N_SPLITS = 3 if QUICK_MODE else 5  # HERE\n",
    "N_REPEATS_TUNING = 1  # repeats used *inside* Optuna tuning\n",
    "N_REPEATS_CV = 1 if QUICK_MODE else 2  # repeats used for post-tuning multi-metric CV\n",
    "\n",
    "# Parallelism (set to 1 if you hit resource limits)\n",
    "NUM_JOBS = -1  # -1 means use all available cores\n",
    "\n",
    "# Seeds for different phases\n",
    "SEED_TUNING = RANDOM_STATE\n",
    "SEED_CV = RANDOM_STATE\n",
    "\n",
    "out_folder = \"coding_exercise_2_colab\"\n",
    "optuna_folder = \"coding_exercise_2_colab/optuna\"\n",
    "tables_folder = \"coding_exercise_2_colab/tables\"\n",
    "out_suffix = \"_cd2\"  # suffix for output files\n",
    "\n",
    "# Optimization objective configuration\n",
    "TUNING_OBJECTIVE = \"mcc\"  # Options: \"mcc\", \"f1\", \"balanced_accuracy\", \"cost_benefit\"\n",
    "\n",
    "# Cost/benefit parameters (only used if TUNING_OBJECTIVE = \"cost_benefit\")\n",
    "# These values define the misclassification costs and benefits for binary classification\n",
    "COST_FP = 2  # Cost of false positive (predicting >50K when actually <=50K)\n",
    "COST_FN = 10  # Cost of false negative (missing >50K earner - costs more!)\n",
    "BENEFIT_TP = 5  # Benefit of true positive (correctly identifying >50K earner)\n",
    "BENEFIT_TN = 5  # Benefit of true negative (correctly identifying <=50K earner)\n",
    "\n",
    "# Threshold optimization configuration\n",
    "USE_OPTIMAL_THRESHOLD = True  # Apply optimal threshold to final predictions\n",
    "THRESHOLD_METRIC = (\n",
    "    \"auto\"  # \"auto\" = use same as TUNING_OBJECTIVE, or specify: \"mcc\", \"f1\", \"balanced_accuracy\", \"cost_benefit\"\n",
    ")\n",
    "\n",
    "# Ensemble configuration\n",
    "ENSEMBLE_METHOD = \"stacking\"  # Options: \"voting\" or \"stacking\"\n",
    "TOP_N_MODELS_FOR_ENSEMBLE = 3  # Number of best models to combine in ensemble\n",
    "\n",
    "# All available models in the system (master list)\n",
    "ALL_MODELS = [\n",
    "    \"DummyMostFreq\",\n",
    "    \"LogisticRegression\",\n",
    "    \"RandomForest\",\n",
    "    \"GradientBoosting\",\n",
    "    \"LightGBM\",\n",
    "    # \"SVC\",\n",
    "    \"MLPClassifier\",\n",
    "]\n",
    "\n",
    "# Sampling strategies for handling class imbalance\n",
    "# See build_pipeline() for sampling options\n",
    "ALL_SAMPLING_METHODS = [\n",
    "    \"none\",\n",
    "    \"smote\",\n",
    "    \"adasyn\",\n",
    "    \"borderline_smote\",\n",
    "    \"random_undersample\",\n",
    "    \"tomek\",\n",
    "    \"smote_tomek\",\n",
    "    \"smote_enn\",\n",
    "]\n",
    "\n",
    "# Mode-specific model configuration\n",
    "# QUICK_MODE: minimal set for fast testing/development\n",
    "# Full mode: comprehensive evaluation of all models\n",
    "if QUICK_MODE:\n",
    "    # Minimal set for fast testing - always include DummyMostFreq as baseline\n",
    "    MODEL_NAMES = [\"DummyMostFreq\", \"RandomForest\", \"LightGBM\", \"GradientBoosting\"]\n",
    "    TUNED_MODELS = [\"LightGBM\"]  # Only tune one model in quick mode\n",
    "    SAMPLING_METHODS = [\"none\", \"smote\", \"adasyn\"]  # Fewer sampling methods in quick mode\n",
    "else:\n",
    "    # Full evaluation with all models\n",
    "    MODEL_NAMES = ALL_MODELS\n",
    "    TUNED_MODELS = [\"LightGBM\", \"MLPClassifier\", \"RandomForest\", \"GradientBoosting\"]\n",
    "    SAMPLING_METHODS = ALL_SAMPLING_METHODS\n",
    "# Validation: ensure TUNED_MODELS is a subset of MODEL_NAMES\n",
    "if not set(TUNED_MODELS).issubset(set(MODEL_NAMES)):\n",
    "    missing = set(TUNED_MODELS) - set(MODEL_NAMES)\n",
    "    raise ValueError(f\"TUNED_MODELS contains models not in MODEL_NAMES: {missing}\")\n",
    "\n",
    "# Alias for use in configuration summary\n",
    "CV_FOLDS = N_SPLITS\n",
    "\n",
    "N_TRIALS_PER_MODEL_DEFAULT = 30\n",
    "N_TRIALS_PER_MODEL = {\n",
    "    \"DummyMostFreq\": 1,\n",
    "    \"LogisticRegression\": 25,\n",
    "    \"RandomForest\": 100 if not QUICK_MODE else 10,\n",
    "    \"GradientBoosting\": 100 if not QUICK_MODE else 10,\n",
    "    \"LightGBM\": 100 if not QUICK_MODE else 15,\n",
    "    \"SVC\": 50 if not QUICK_MODE else 10,\n",
    "    \"MLPClassifier\": 50 if not QUICK_MODE else 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (install required packages automatically in Colab if needed)\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    # Install required packages in Colab\n",
    "    import subprocess\n",
    "\n",
    "    packages = [\"optuna\", \"imbalanced-learn\", \"lightgbm\"]\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import os\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "# Imbalanced-learn imports\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, BorderlineSMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_param_importances\n",
    "from sklearn.calibration import CalibrationDisplay, calibration_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    make_scorer,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, learning_curve, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "os.makedirs(optuna_folder, exist_ok=True)\n",
    "os.makedirs(tables_folder, exist_ok=True)\n",
    "\n",
    "# Configure sklearn to output pandas DataFrames from transformers\n",
    "# This fixes LightGBM warnings about feature names by preserving column names\n",
    "# throughout the pipeline. Requires sklearn >= 1.2.\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a50bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Adult Census Income dataset (same source as Coding Exercise 1)\n",
    "DATA_URL = \"https://github.com/medmaca/shared_data/raw/8a3fea5467ec68b17fd8369c6f77f8016b1ed5f8/Datasets/Kaggle/adult_census_income/adult.csv.zip\"\n",
    "\n",
    "adult_ci_df = pd.read_csv(DATA_URL, compression=\"zip\")\n",
    "adult_ci_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734abf8",
   "metadata": {},
   "source": [
    "## 1. Minimal dataset checks (see Coding Exercise 1 section 4 for full EDA)\n",
    "\n",
    "### Learning Objectives:\n",
    "- Verify data loaded correctly\n",
    "- Understand the extent of class imbalance\n",
    "- Recognize why standard accuracy is misleading for imbalanced datasets\n",
    "\n",
    "In Coding Exercise 1 we performed detailed exploratory data analysis (EDA).\n",
    "Here we only do minimal sanity checks and then move on to **pipelines**, **metrics** and **hyperparameter tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info and class distribution\n",
    "adult_ci_df.info()\n",
    "\n",
    "target_col = \"income\"  # same as Coding Exercise 1\n",
    "\n",
    "class_counts = adult_ci_df[target_col].value_counts().sort_index()\n",
    "class_props = adult_ci_df[target_col].value_counts(normalize=True).sort_index()\n",
    "print(f\"\\nClass counts:{class_counts}\")\n",
    "print(f\"\\nClass proportions:{class_props}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa40bea",
   "metadata": {},
   "source": [
    "### Understanding Class Imbalance\n",
    "\n",
    "The dataset shows approximately 75% of samples belong to the <=50K class and 25% to the >50K class.\n",
    "This 3:1 imbalance means:\n",
    "\n",
    "- A naive classifier that always predicts \"<=50K\" would achieve 75% accuracy\n",
    "- Accuracy alone is misleading and insufficient for model evaluation\n",
    "- We need metrics that account for both classes equally\n",
    "- We should consider resampling techniques to balance the training data\n",
    "\n",
    "**Common Pitfall:** Using accuracy as the primary metric for imbalanced data can lead to models that simply predict the majority class and appear to perform well whilst being useless in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f797f",
   "metadata": {},
   "source": [
    "## 2. Understanding Classification Metrics for Imbalanced Data\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the confusion matrix and derived metrics\n",
    "- Learn which metrics are appropriate for imbalanced classification\n",
    "- Understand the Matthews Correlation Coefficient (MCC)\n",
    "- Learn how to incorporate misclassification costs into model evaluation\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "All binary classification metrics derive from the confusion matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "& \\text{Predicted Negative} & \\text{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\text{Actual Negative} & TN & FP \\\\\n",
    "\\text{Actual Positive} & FN & TP \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP (True Positives)**: Correctly predicted positive class\n",
    "- **TN (True Negatives)**: Correctly predicted negative class\n",
    "- **FP (False Positives)**: Incorrectly predicted positive (Type I error)\n",
    "- **FN (False Negatives)**: Incorrectly predicted negative (Type II error)\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "& \\text{Predicted Negative} & \\text{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\text{Actual Negative} & - & \\text{Type I Error} \\\\\n",
    "\\text{Actual Positive} & \\text{Type II Error} & - \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Key Metrics Explained\n",
    "See https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "**1. Accuracy = (TP + TN) / (TP + TN + FP + FN)**\n",
    "- Proportion of correct predictions\n",
    "- **Problem:** Misleading with imbalanced data\n",
    "- **When to use:** Only when classes are balanced and errors equally costly\n",
    "\n",
    "**2. Precision = TP / (TP + FP)**\n",
    "- Of all positive predictions, how many were correct?\n",
    "- **Use case:** When false positives are costly (e.g., spam detection)\n",
    "- **Trade-off:** Can be high by predicting positive rarely\n",
    "\n",
    "**3. Sensitivity = TP / (TP + FN)**\n",
    "- `Also called Recall` or True Positive Rate (TPR)\n",
    "- Of all actual positives, how many did we find?\n",
    "- **Use case:** When false negatives are costly (e.g., disease detection)\n",
    "- **Trade-off:** Can be high by predicting positive frequently\n",
    "\n",
    "**4. Specificity = TN / (TN + FP)**\n",
    "- Also called True Negative Rate (TNR)\n",
    "- Of all actual negatives, how many did we correctly identify?\n",
    "- **Use case:** Complements recall; important when true negatives matter\n",
    "\n",
    "**5. F1 Score = 2 × (Precision × Recall) / (Precision + Recall)**\n",
    "- https://en.wikipedia.org/wiki/F-score\n",
    "- Harmonic mean of precision and recall\n",
    "- **Advantage:** Balances precision and recall\n",
    "- **Limitation:** Doesn't account for true negatives\n",
    "\n",
    "**6. Balanced Accuracy = (Recall + Specificity) / 2**\n",
    "- Average of per-class accuracies\n",
    "- **Advantage:** Handles imbalanced data well\n",
    "- **Use case:** When both classes are equally important\n",
    "\n",
    "**7. Matthews Correlation Coefficient (MCC)**\n",
    "- [MCC](https://en.wikipedia.org/wiki/Phi_coefficient)\n",
    "\n",
    "$$\n",
    "\\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "$$\n",
    "\n",
    "- Range: -1 (total disagreement) to +1 (perfect prediction), 0 = random\n",
    "- **Advantages:**\n",
    "  - Uses all four confusion matrix values\n",
    "  - Balanced measure even with imbalanced classes\n",
    "  - More informative than F1 for imbalanced data\n",
    "  - Directly interpretable as a correlation coefficient\n",
    "- **Disadvantages:**\n",
    "  - Less intuitive than accuracy or F1\n",
    "  - Requires understanding of correlation coefficients\n",
    "- **When to use:** Imbalanced classification where all four outcomes matter\n",
    "\n",
    "**8. ROC AUC (Area Under Receiver Operating Characteristic)**\n",
    "- https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "- Plots True Positive Rate vs False Positive Rate across thresholds\n",
    "- **Advantage:** Threshold-independent, handles imbalance reasonably\n",
    "- **Limitation:** Optimistic for severe imbalance\n",
    "\n",
    "**9. PR AUC (Area Under Precision-Recall)**\n",
    "- Plots Precision vs Recall across thresholds\n",
    "- **Advantage:** More informative than ROC AUC for imbalanced data\n",
    "- **Use case:** When positive class is rare and important\n",
    "\n",
    "### Why MCC is Often the Best Single Metric\n",
    "\n",
    "For this tutorial, we use MCC as our primary optimization metric because:\n",
    "1. It's specifically designed for imbalanced datasets\n",
    "2. It considers all four confusion matrix cells equally\n",
    "3. It provides a single, interpretable correlation value\n",
    "4. It's been shown to be more reliable than F1 for imbalanced problems\n",
    "\n",
    "However, we'll compute many metrics to understand model behaviour comprehensively.\n",
    "\n",
    "**Common Pitfall:** Relying on a single metric. Always examine multiple metrics and the confusion matrix to understand model behaviour fully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8e1dd",
   "metadata": {},
   "source": [
    "## 3. Cost-Benefit Analysis for Cost-Aware ML\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand how to incorporate misclassification costs into model evaluation\n",
    "- Learn the difference between symmetric and asymmetric cost structures\n",
    "- Extend cost-benefit analysis to multiclass problems\n",
    "\n",
    "### The Cost Context\n",
    "\n",
    "In real-world applications, different types of errors have different costs:\n",
    "- **False Positive (FP):** Predicting >50K when actually <=50K - might waste resources on unnecessary interventions\n",
    "- **False Negative (FN):** Predicting <=50K when actually >50K - might miss valuable opportunities\n",
    "\n",
    "Traditional metrics (accuracy, F1, MCC) treat all errors equally. Cost-benefit analysis allows us to optimize for real-world objectives.\n",
    "\n",
    "### Binary Classification Cost Matrix\n",
    "\n",
    "For binary problems, we define a 2×2 cost matrix:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    " & \\text{Predicted 0} & \\text{Predicted 1} \\\\\n",
    "\\hline\n",
    "\\text{Actual 0} & \\text{BENEFIT}_{\\mathrm{TN}} & \\text{COST}_{\\mathrm{FP}} \\\\\n",
    "\\text{Actual 1} & \\text{COST}_{\\mathrm{FN}} & \\text{BENEFIT}_{\\mathrm{TP}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Example for Adult Census Income:**\n",
    "- COST_FP = 1 (low cost - false alarm)\n",
    "- COST_FN = 10 (high cost - missing a high earner)\n",
    "- BENEFIT_TP = 5 (benefit of correctly identifying high earner)\n",
    "- BENEFIT_TN = 0 (no special benefit for correct low earner prediction)\n",
    "\n",
    "This asymmetry reflects that missing a high earner (FN) is 10× more costly than a false alarm (FP).\n",
    "\n",
    "### Multiclass Extension\n",
    "\n",
    "The cost matrix naturally extends to multiclass problems as an N×N matrix where:\n",
    "- Rows represent true classes\n",
    "- Columns represent predicted classes\n",
    "- Diagonal elements are benefits (correct predictions)\n",
    "- Off-diagonal elements are costs (misclassification costs)\n",
    "\n",
    "**Example for 3-class income problem (Low, Medium, High):**\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccc}\n",
    " & \\text{Pred Low} & \\text{Pred Med} & \\text{Pred High} \\\\\n",
    "\\hline\n",
    "\\text{True Low}  & 0  & 5  & 20 \\\\\n",
    "\\text{True Med}  & 3  & 0  & 10 \\\\\n",
    "\\text{True High} & 50 & 15 & 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Here, misclassifying a High earner as Low (cost=50) is catastrophic, whilst misclassifying Low as Medium (cost=5) is minor.\n",
    "\n",
    "### When to Use Cost-Benefit Analysis\n",
    "\n",
    "Use cost-benefit optimization when:\n",
    "1. Different errors have quantifiable, different costs\n",
    "2. Domain experts can define cost/benefit values\n",
    "3. The goal is to maximize profit or minimize loss rather than optimize a metric\n",
    "\n",
    "**Common Pitfall:** Using arbitrary cost values. Work with domain experts to define realistic costs that reflect true real-world impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4effe",
   "metadata": {},
   "source": [
    "## 4. Handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing data markers and define feature types\n",
    "# In this dataset, missing values are encoded as the literal string '?' in several categorical columns.\n",
    "\n",
    "# Replace '?' with NaN so that SimpleImputer can handle them inside the pipeline\n",
    "adult_ci_df = adult_ci_df.replace(\"?\", np.nan)\n",
    "\n",
    "feature_cols = [c for c in adult_ci_df.columns if c != target_col]\n",
    "categorical_features = [c for c in feature_cols if adult_ci_df[c].dtype == \"object\"]\n",
    "numeric_features = [c for c in feature_cols if adult_ci_df[c].dtype != \"object\"]\n",
    "\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "print(\"Numeric features:\", numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e2f6e",
   "metadata": {},
   "source": [
    "## 5. Encode target and create a single held-out test set\n",
    "\n",
    "use [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split into train and test sets, whilst maintaining class proportions with the `stratify` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target and create a single held-out test set\n",
    "# We use LabelBinarizer so we can recover the original class labels later for plots.\n",
    "lb = LabelBinarizer()\n",
    "adult_ci_df[\"target\"] = lb.fit_transform(adult_ci_df[target_col].str.strip()).ravel()\n",
    "print(\"Target classes (lb.classes_):\", lb.classes_)\n",
    "\n",
    "X = adult_ci_df[feature_cols].copy()\n",
    "y = adult_ci_df[\"target\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "print(\"Train class distribution:\", y_train.value_counts(normalize=True).to_dict())\n",
    "print(\"Test class distribution:\", y_test.value_counts(normalize=True).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8cf9a",
   "metadata": {},
   "source": [
    "## 6. Handling Class Imbalance with Resampling Techniques\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand why class imbalance is problematic\n",
    "- Learn different resampling strategies (oversampling, undersampling, hybrid)\n",
    "- Understand when to apply resampling (training only, never test data)\n",
    "- Learn how to integrate resampling into pipelines safely\n",
    "\n",
    "### The Problem with Class Imbalance\n",
    "\n",
    "With imbalanced data, machine learning models often:\n",
    "1. **Bias towards the majority class** - models learn to predict the common class\n",
    "2. **Poor minority class performance** - fail to learn patterns in the rare class\n",
    "3. **Misleading accuracy** - high accuracy by predicting only the majority class\n",
    "\n",
    "### Resampling Strategies\n",
    "- see https://imbalanced-learn.org/stable/\n",
    "**1. Oversampling (Increasing Minority Class)**\n",
    "https://imbalanced-learn.org/stable/over_sampling.html\n",
    "\n",
    "- **Random Oversampling**: Duplicate minority class samples\n",
    "  - Simple but risks overfitting\n",
    "\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Create synthetic samples by interpolating between minority class neighbours\n",
    "  - Advantages: Reduces overfitting vs random duplication\n",
    "  - Disadvantages: Can create noise in overlap regions\n",
    "\n",
    "- **ADASYN (Adaptive Synthetic Sampling)**: Like SMOTE but focuses on difficult-to-learn examples\n",
    "  - Advantages: More samples near decision boundary\n",
    "  - Disadvantages: More complex, can amplify noise\n",
    "\n",
    "- **BorderlineSMOTE**: Only generates samples near the decision boundary\n",
    "  - Advantages: More focused than SMOTE\n",
    "  - Use case: When minority class has distinct regions\n",
    "\n",
    "**2. Undersampling (Reducing Majority Class)**\n",
    "https://imbalanced-learn.org/stable/under_sampling.html\n",
    "\n",
    "- **Random Undersampling**: Randomly remove majority class samples\n",
    "  - Fast but loses potentially useful information\n",
    "\n",
    "- **TomekLinks**: Remove majority class samples that are close to minority class\n",
    "  - Advantages: Cleans decision boundary\n",
    "  - Disadvantages: Removes very little data\n",
    "\n",
    "**3. Hybrid Methods (Combining Both)**\n",
    "https://imbalanced-learn.org/stable/combine.html\n",
    "- **SMOTETomek**: Apply SMOTE then clean with Tomek links\n",
    "  - Advantages: Oversample minority, clean noisy samples\n",
    "  - Best for: Moderate imbalance with noisy data\n",
    "\n",
    "- **SMOTEENN**: Apply SMOTE then clean with Edited Nearest Neighbours\n",
    "  - More aggressive cleaning than SMOTETomek\n",
    "\n",
    "### Decision Guide for Imbalance Ratio\n",
    "\n",
    "- **Mild imbalance (< 4:1)**: Try `class_weight='balanced'` first\n",
    "- **Moderate imbalance (4:1 to 10:1)**: SMOTE or SMOTETomek\n",
    "- **Severe imbalance (> 10:1)**: ADASYN, hybrid methods, or consider anomaly detection\n",
    "\n",
    "### Critical Rules\n",
    "\n",
    "1. **Only resample training data** - never resample test/validation data\n",
    "2. **Resample after train/test split** - prevents data leakage\n",
    "3. **Use imblearn.pipeline.Pipeline** - ensures resampling happens in cross-validation\n",
    "4. **Monitor both classes** - use appropriate metrics (MCC, F1, balanced accuracy)\n",
    "\n",
    "**Common Pitfall:** Applying resampling before train/test split causes data leakage, as synthetic test samples may be based on training data, leading to overly optimistic performance estimates.\n",
    "\n",
    "### Implementation in this Tutorial\n",
    "\n",
    "We integrate resampling into our hyperparameter search, allowing Optuna to select the best strategy for each model. This is done using `imblearn.pipeline.Pipeline`, which properly handles resampling within cross-validation folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b81921",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 7. Utility Functions for Metrics and Model Evaluation\n",
    "\n",
    "### Learning Objectives:\n",
    "- Implement custom metric functions\n",
    "- Create a threshold-adjusted classifier wrapper\n",
    "- Build reusable cost-benefit scoring functions\n",
    "\n",
    "We define several utility functions that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"Calculate specificity (true negative rate) for binary classification.\n",
    "\n",
    "    Specificity measures the proportion of actual negatives that are correctly identified.\n",
    "    Also known as selectivity or true negative rate (TNR).\n",
    "\n",
    "    Not implemented in sklearn by default.  Hence implemented manually.\n",
    "\n",
    "    Formula: TN / (TN + FP)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0 or 1).\n",
    "    y_pred : array-like\n",
    "        Predicted binary labels (0 or 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Specificity score in range [0, 1], or np.nan if undefined.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Returns np.nan if there are no negative samples (TN + FP = 0) or if\n",
    "    confusion matrix is not 2x2 (non-binary case).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape != (2, 2):\n",
    "        return np.nan\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    denom = tn + fp\n",
    "    return tn / denom if denom > 0 else np.nan\n",
    "\n",
    "\n",
    "def cost_benefit_score(y_true, y_pred, cost_matrix, normalize=True):\n",
    "    \"\"\"Calculate cost-benefit score using a cost matrix.\n",
    "\n",
    "    Works for both binary and multiclass classification. The score is calculated as\n",
    "    the sum of (confusion_matrix × cost_matrix). Higher scores are better.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True class labels.\n",
    "    y_pred : array-like\n",
    "        Predicted class labels.\n",
    "    cost_matrix : array-like, shape (n_classes, n_classes)\n",
    "        Cost/benefit matrix where entry [i, j] represents the value when:\n",
    "        - True class is i (row index)\n",
    "        - Predicted class is j (column index)\n",
    "\n",
    "        **Convention:**\n",
    "        - POSITIVE values = benefits (good outcomes: TN, TP)\n",
    "        - NEGATIVE values = costs (bad outcomes: FP, FN)\n",
    "    normalize : bool, default=True\n",
    "        If True, normalize the score to the range [0, 1] where:\n",
    "        - 0 = worst possible score (complete misclassification)\n",
    "        - 1 = best possible score (perfect classification)\n",
    "        If False, return the per-sample average cost-benefit score.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        Cost-benefit score. If normalize=True, returns value in [0, 1].\n",
    "        If normalize=False, returns per-sample average score where higher\n",
    "        (more positive) is better. Per-sample averaging ensures scores are\n",
    "        comparable across different fold sizes during cross-validation.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    For binary classification (negative class=0, positive class=1), cost_matrix format:\n",
    "\n",
    "        cost_matrix = [[BENEFIT_TN,  -COST_FP ],\n",
    "                       [-COST_FN,     BENEFIT_TP]]\n",
    "\n",
    "    Confusion matrix mapping (sklearn convention):\n",
    "                    Predicted\n",
    "                    0 (Neg)    1 (Pos)\n",
    "        Actual 0    TN         FP       ← Row 0: Actual negatives\n",
    "               1    FN         TP       ← Row 1: Actual positives\n",
    "\n",
    "    Score = TNxBENEFIT_TN + FPx(-COST_FP) + FNx(-COST_FN) + TPxBENEFIT_TP\n",
    "          = (TN_benefit + TP_benefit) - (FP_cost + FN_cost)\n",
    "\n",
    "    Normalization:\n",
    "        normalized = (raw - worst) / (best - worst)\n",
    "        where:\n",
    "        - best = score with perfect predictions (all diagonal)\n",
    "        - worst = score with worst predictions (all off-diagonal, maximizing costs)\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> # Binary case: FN costs 10, FP costs 1, TP benefit 5, TN no benefit\n",
    "    >>> cost_matrix = np.array([[0, -1],    # [TN_benefit, -FP_cost]\n",
    "    ...                         [-10, 5]])  # [-FN_cost, TP_benefit]\n",
    "    >>>\n",
    "    >>> y_true = [0, 0, 1, 1, 1]\n",
    "    >>> y_pred = [0, 1, 1, 1, 0]  # TN=1, FP=1, FN=1, TP=2\n",
    "    >>> cost_benefit_score(y_true, y_pred, cost_matrix, normalize=False)\n",
    "    -0.2  # = (1x0 + 1x(-1) + 1x(-10) + 2x5) / 5 = -1 / 5 = -0.2\n",
    "    >>>\n",
    "    >>> # Normalized version\n",
    "    >>> cost_benefit_score(y_true, y_pred, cost_matrix, normalize=True)\n",
    "    0.97  # (raw - worst) / (best - worst) = (-1 - (-22)) / (10 - (-22)) = 21/32\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cost_matrix = np.array(cost_matrix)\n",
    "\n",
    "    if cm.shape != cost_matrix.shape:\n",
    "        raise ValueError(f\"Confusion matrix shape {cm.shape} doesn't match cost matrix shape {cost_matrix.shape}\")\n",
    "\n",
    "    # Element-wise multiply confusion matrix by cost/benefit values, then sum\n",
    "    # Positive values in cost_matrix = benefits, negative = costs\n",
    "    # Higher return value = better performance\n",
    "    raw_score = np.sum(cm * cost_matrix)\n",
    "\n",
    "    # Normalize by sample count to get per-sample score (makes scores comparable\n",
    "    # across different fold sizes / datasets)\n",
    "    n_samples = cm.sum()\n",
    "    per_sample_score = raw_score / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "    if not normalize:\n",
    "        return per_sample_score\n",
    "\n",
    "    # Calculate best and worst possible scores for normalization\n",
    "    # Best: all predictions are correct (diagonal of confusion matrix)\n",
    "    class_counts = cm.sum(axis=1)  # true class counts\n",
    "\n",
    "    # Best score: perfect predictions - all samples on diagonal\n",
    "    best_score = np.sum(class_counts * np.diag(cost_matrix))\n",
    "\n",
    "    # Worst score: maximize costs by predicting the worst class for each true class\n",
    "    # For each true class, find the prediction that gives the worst (most negative) value\n",
    "    worst_score = 0.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        # Find the worst prediction for true class i (excluding correct prediction)\n",
    "        # Convert to float to allow np.inf assignment\n",
    "        costs_for_class = cost_matrix[i, :].astype(float).copy()\n",
    "        costs_for_class[i] = np.inf  # Exclude correct prediction\n",
    "        worst_pred = np.argmin(costs_for_class)\n",
    "        worst_score += class_counts[i] * cost_matrix[i, worst_pred]\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    if best_score == worst_score:\n",
    "        return 1.0 if raw_score >= best_score else 0.0\n",
    "\n",
    "    normalized_score = (raw_score - worst_score) / (best_score - worst_score)\n",
    "    return float(np.clip(normalized_score, 0.0, 1.0))\n",
    "\n",
    "\n",
    "class ThresholdedClassifier:\n",
    "    \"\"\"Wrapper that applies a custom decision threshold to a probabilistic classifier.\n",
    "\n",
    "    Standard classifiers use a 0.5 threshold for binary classification. This wrapper\n",
    "    allows optimising the threshold for specific metrics or objectives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object\n",
    "        A fitted classifier that implements `predict_proba`.\n",
    "    threshold : float, default=0.5\n",
    "        Decision threshold in range [0, 1]. Samples with predicted probability\n",
    "        >= threshold are classified as positive class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimator : estimator object\n",
    "        The wrapped classifier.\n",
    "    threshold : float\n",
    "        The decision threshold.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    >>> # Use 0.3 threshold to favour recall over precision\n",
    "    >>> thresholded_clf = ThresholdedClassifier(clf, threshold=0.3)\n",
    "    >>> predictions = thresholded_clf.predict(X_test)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This wrapper assumes binary classification and uses the probability of the\n",
    "    positive class (class 1) for thresholding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator, threshold=0.5):\n",
    "        \"\"\"Initialize the thresholded classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : estimator object\n",
    "            A fitted classifier with predict_proba method.\n",
    "        threshold : float, default=0.5\n",
    "            Decision threshold in [0, 1].\n",
    "        \"\"\"\n",
    "        self.estimator = estimator\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels using the custom threshold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Predicted class labels (0 or 1).\n",
    "        \"\"\"\n",
    "        proba = self.estimator.predict_proba(X)[:, 1]\n",
    "        return (proba >= self.threshold).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : array, shape (n_samples, 2)\n",
    "            Class probabilities.\n",
    "        \"\"\"\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ThresholdedClassifier(estimator={self.estimator}, threshold={self.threshold})\"\n",
    "\n",
    "\n",
    "# Create the cost matrix for binary classification based on configuration\n",
    "# Convention: Benefits are positive, costs are negative\n",
    "# Matrix format: [[TN_benefit, -FP_cost], [-FN_cost, TP_benefit]]\n",
    "COST_MATRIX = np.array([[BENEFIT_TN, -COST_FP], [-COST_FN, BENEFIT_TP]])\n",
    "\n",
    "print(\"Cost matrix for cost-benefit analysis:\")\n",
    "print(COST_MATRIX)\n",
    "print(f\"Interpretation: FN costs {COST_FN}, FP costs {COST_FP}, TP benefit {BENEFIT_TP}, TN benefit {BENEFIT_TN}\")\n",
    "print(\"(Negative values = costs, positive values = benefits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595dae2",
   "metadata": {},
   "source": [
    "## 8. Preprocessing with ColumnTransformer and Pipeline\n",
    "\n",
    "### Learning Objectives:\n",
    "- Build leak-proof preprocessing pipelines\n",
    "- Understand why preprocessing must be inside the pipeline\n",
    "- Integrate resampling strategies safely\n",
    "- Create modular, reusable pipeline components\n",
    "\n",
    "We use:\n",
    "- [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) to handle missing values,\n",
    "- [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) for numeric features (robust to outliers),\n",
    "- [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) with `handle_unknown=\"ignore\"` for categoricals,\n",
    "- [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to combine these,\n",
    "- and wrap everything in a single [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) with a classifier as the final step.\n",
    "\n",
    "**Why Pipelines Prevent Data Leakage:**\n",
    "- Scalers, imputers, and encoders are *fitted* on training data only\n",
    "- The same transformations are *applied* to validation/test data\n",
    "- Without pipelines, it's easy to accidentally fit on all data (leakage)\n",
    "\n",
    "Note that for simplicity here we're using the `RobustScaler` for all numeric data, in Coding Exercise 1 (section 6) we used QuantileTransformer for variables that had an IQR of zero.  However, since the `RobustScaler` handles IQR=0 cases gracefully (it just scales by 1), this is acceptable for our purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric and categorical preprocessing pipelines\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        # sparse_output=False is required when using sklearn.set_config(transform_output=\"pandas\")\n",
    "        # because pandas DataFrames don't support sparse matrices\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def build_pipeline(estimator, sampling_strategy=\"none\"):\n",
    "    \"\"\"Build a full preprocessing + optional resampling + model pipeline.\n",
    "\n",
    "    This function creates a complete ML pipeline that:\n",
    "    1. Preprocesses data (imputation, scaling, encoding)\n",
    "    2. Optionally resamples training data to handle class imbalance\n",
    "    3. Applies the final classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object\n",
    "        An sklearn-compatible classifier (e.g., RandomForestClassifier).\n",
    "    sampling_strategy : str, default=\"none\"\n",
    "        The resampling strategy to apply. Options:\n",
    "        - \"none\": No resampling\n",
    "        - \"smote\": SMOTE oversampling\n",
    "        - \"adasyn\": ADASYN oversampling\n",
    "        - \"borderline_smote\": BorderlineSMOTE oversampling\n",
    "        - \"random_oversample\": Random oversampling\n",
    "        - \"random_undersample\": Random undersampling\n",
    "        - \"tomek\": Tomek links undersampling\n",
    "        - \"smote_tomek\": SMOTE + Tomek links (hybrid)\n",
    "        - \"smote_enn\": SMOTE + ENN (hybrid)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pipeline : Pipeline or ImbPipeline\n",
    "        A complete pipeline including preprocessing, optional resampling, and classifier.\n",
    "        Returns imblearn.pipeline.Pipeline if sampling is used, else sklearn.pipeline.Pipeline.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses imblearn.pipeline.Pipeline when sampling is specified to ensure resampling\n",
    "    happens correctly within cross-validation folds. This prevents data leakage.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> clf = RandomForestClassifier()\n",
    "    >>> pipe = build_pipeline(clf, sampling_strategy=\"smote\")\n",
    "    >>> pipe.fit(X_train, y_train)\n",
    "    \"\"\"\n",
    "    steps = [(\"preprocess\", preprocessor)]\n",
    "\n",
    "    # Add sampling step if requested\n",
    "    if sampling_strategy != \"none\":\n",
    "        if sampling_strategy == \"smote\":\n",
    "            sampler = SMOTE(random_state=RANDOM_STATE)\n",
    "        elif sampling_strategy == \"adasyn\":\n",
    "            sampler = ADASYN(random_state=RANDOM_STATE)\n",
    "        elif sampling_strategy == \"borderline_smote\":\n",
    "            sampler = BorderlineSMOTE(random_state=RANDOM_STATE)\n",
    "        elif sampling_strategy == \"random_oversample\":\n",
    "            sampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "        elif sampling_strategy == \"random_undersample\":\n",
    "            sampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "        elif sampling_strategy == \"tomek\":\n",
    "            sampler = TomekLinks()\n",
    "        elif sampling_strategy == \"smote_tomek\":\n",
    "            sampler = SMOTETomek(random_state=RANDOM_STATE)\n",
    "        elif sampling_strategy == \"smote_enn\":\n",
    "            sampler = SMOTEENN(random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling_strategy: {sampling_strategy}\")\n",
    "\n",
    "        steps.append((\"sampler\", sampler))\n",
    "\n",
    "    steps.append((\"clf\", estimator))\n",
    "\n",
    "    # Use imblearn Pipeline if sampling is used, otherwise sklearn Pipeline\n",
    "    pipeline_class = ImbPipeline if sampling_strategy != \"none\" else Pipeline\n",
    "    return pipeline_class(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dec1c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Quick visualization of the preprocessor pipeline\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c79c99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 9. Model zoo and hyperparameter search spaces\n",
    "\n",
    "### Learning Objectives:\n",
    "- Set up multiple models for comparison\n",
    "- Define hyperparameter search spaces for Optuna\n",
    "- Integrate sampling strategies as hyperparameters\n",
    "- Understand different model families and their strengths\n",
    "\n",
    "We re-use the same 5 models from Coding Exercise 1 and add two additional models:\n",
    "- [`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) (baseline),\n",
    "- [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),\n",
    "- [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html),\n",
    "- [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html),\n",
    "- [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) with `probability=True` so we can compute ROC/PR AUC.\n",
    "- [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) as a simple feedforward neural network classifier.\n",
    "- [`LGBMClassifier`](https://lightgbm.readthedocs.io/) from LightGBM for fast gradient boosting.\n",
    "\n",
    "For each model we define:\n",
    "- a *base* configuration used when not tuning,\n",
    "- and an [Optuna](https://optuna.org/) **search space** describing how to sample hyperparameters.\n",
    "\n",
    "**Common Pitfall:** Using the same model with slightly different hyperparameters in an ensemble provides minimal diversity. Consider different algorithm families for better ensemble performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e7ec6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def create_base_estimator(model_name, params=None):\n",
    "    \"\"\"Return an sklearn classifier with sensible defaults, optionally updated with params.\"\"\"\n",
    "    params = dict(params or {})\n",
    "\n",
    "    if model_name == \"DummyMostFreq\":\n",
    "        base = {\"strategy\": \"most_frequent\"}\n",
    "        base.update(params)\n",
    "        return DummyClassifier(**base)\n",
    "\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        # Sensible, fairly robust base configuration for this dataset\n",
    "        base = {\n",
    "            \"max_iter\": 10000,\n",
    "            \"C\": 15,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"n_jobs\": NUM_JOBS,\n",
    "            \"tol\": 1e-5,\n",
    "            \"solver\": \"lbfgs\",  # default; may be overridden by tuning\n",
    "            \"penalty\": \"l2\",\n",
    "            # l1_ratio will only be set when using penalty='elasticnet'\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        }\n",
    "        base.update(params)\n",
    "        return LogisticRegression(**base)\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "        base = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": None,\n",
    "            \"min_samples_split\": 2,\n",
    "            \"min_samples_leaf\": 1,\n",
    "            \"max_features\": \"sqrt\",\n",
    "            \"n_jobs\": NUM_JOBS,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        }\n",
    "        base.update(params)\n",
    "        return RandomForestClassifier(**base)\n",
    "\n",
    "    if model_name == \"GradientBoosting\":\n",
    "        base = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 3,\n",
    "            \"subsample\": 1.0,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"min_weight_fraction_leaf\": 0.0,\n",
    "        }\n",
    "\n",
    "        base.update(params)\n",
    "        return GradientBoostingClassifier(**base)\n",
    "\n",
    "    if model_name == \"SVC\":\n",
    "        base = {\n",
    "            \"kernel\": \"rbf\",\n",
    "            \"gamma\": \"scale\",\n",
    "            \"C\": 1.0,\n",
    "            \"probability\": True,  # needed for ROC/PR AUC\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        }\n",
    "        base.update(params)\n",
    "        return SVC(**base)\n",
    "\n",
    "    if model_name == \"MLPClassifier\":\n",
    "        # Simple feedforward neural network; early_stopping keeps training efficient.\n",
    "        base = {\n",
    "            \"solver\": \"adam\",\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_sizes\": (100,),\n",
    "            \"alpha\": 1e-4,\n",
    "            \"learning_rate_init\": 0.001,\n",
    "            \"max_iter\": 200,\n",
    "            \"early_stopping\": True,\n",
    "            \"n_iter_no_change\": 10,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        }\n",
    "\n",
    "        if (\n",
    "            \"hidden_layer_sizes\" in params\n",
    "            and isinstance(params[\"hidden_layer_sizes\"], str)\n",
    "            and \"-\" in params[\"hidden_layer_sizes\"]\n",
    "        ):\n",
    "            # For when hidden_layer_sizes is passed as a string like \"64-32\"\n",
    "            hidden_layer_sizes = tuple(map(int, params[\"hidden_layer_sizes\"].split(\"-\")))\n",
    "            params[\"hidden_layer_sizes\"] = hidden_layer_sizes\n",
    "\n",
    "        base.update(params)\n",
    "        return MLPClassifier(**base)\n",
    "\n",
    "    if model_name == \"LightGBM\":\n",
    "        # LightGBM classifier with sensible defaults\n",
    "        # Note: n_jobs=1 to avoid nested parallelism when cross_val_score uses n_jobs=-1\n",
    "        base = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": -1,  # no limit\n",
    "            \"num_leaves\": 31,\n",
    "            \"subsample\": 1.0,\n",
    "            \"colsample_bytree\": 1.0,\n",
    "            \"reg_alpha\": 0.0,\n",
    "            \"reg_lambda\": 0.0,\n",
    "            \"min_child_samples\": 20,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": 1,  # Avoid nested parallelism with cross_val_score\n",
    "            \"verbose\": -1,  # suppress warnings\n",
    "        }\n",
    "        base.update(params)\n",
    "        return lgb.LGBMClassifier(**base)\n",
    "\n",
    "    raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "\n",
    "def suggest_params_for_model(trial, model_name, sample_methods=SAMPLING_METHODS):\n",
    "    \"\"\"Define Optuna search spaces for each model, including sampling strategies.\n",
    "\n",
    "    The idea is:\n",
    "    1. Start from the model's *base* parameters via ``create_base_estimator``.\n",
    "    2. Suggest a sampling strategy for handling class imbalance.\n",
    "    3. Override model-specific parameters with Optuna suggestions.\n",
    "\n",
    "    For ``LogisticRegression``, we also enforce solver/penalty/l1_ratio compatibility:\n",
    "      * lbfgs, newton-cg, newton-cholesky, sag: L2 only (no l1_ratio)\n",
    "      * liblinear: L1 or L2 only (l1_ratio in {0, 1}, used only to pick penalty)\n",
    "      * saga: L1, L2 or Elastic-Net (0 <= l1_ratio <= 1; only pass l1_ratio when elasticnet)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object for suggesting hyperparameters.\n",
    "    model_name : str\n",
    "        Name of the model to tune.\n",
    "    sample_methods : list of str, optional\n",
    "        List of sampling strategies to consider, by default SAMPLING_METHODS.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (dict, str)\n",
    "        - dict: Model hyperparameters\n",
    "        - str: Sampling strategy name\n",
    "    \"\"\"\n",
    "    # Start from base estimator params so non-optimised parameters come from a central default\n",
    "    base_est = create_base_estimator(model_name)\n",
    "    base_params = base_est.get_params(deep=False)\n",
    "\n",
    "    # Suggest sampling strategy for all models (except Dummy)\n",
    "    if model_name == \"DummyMostFreq\":\n",
    "        sampling_strategy = \"none\"\n",
    "    else:\n",
    "        sampling_strategy = trial.suggest_categorical(\"sampling_strategy\", sample_methods)\n",
    "\n",
    "    if model_name == \"DummyMostFreq\":\n",
    "        # No tuning; just use base parameters\n",
    "        return base_params, sampling_strategy\n",
    "\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        params = dict(base_params)\n",
    "\n",
    "        # Shared hyperparameters to tune\n",
    "        c_param = trial.suggest_float(\"C\", 1e-3, 1e3, log=True)\n",
    "        params[\"C\"] = c_param\n",
    "\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "        params[\"class_weight\"] = class_weight\n",
    "\n",
    "        solver = trial.suggest_categorical(\n",
    "            \"solver\",\n",
    "            # [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"],\n",
    "            [\"lbfgs\"],  # lbfgs is a good fast default for this dataset\n",
    "        )\n",
    "        params[\"solver\"] = solver\n",
    "\n",
    "        # Solver-specific constraints on penalty / l1_ratio\n",
    "        if solver in [\"lbfgs\", \"newton-cg\", \"newton-cholesky\", \"sag\"]:\n",
    "            # Only L2 penalty is supported; ensure no l1_ratio is passed\n",
    "            params[\"penalty\"] = \"l2\"\n",
    "            params.pop(\"l1_ratio\", None)\n",
    "            return params, sampling_strategy\n",
    "\n",
    "        if solver == \"liblinear\":\n",
    "            # liblinear supports L1 or L2, but not Elastic-Net; restrict l1_ratio to {0, 1}.\n",
    "            # We use l1_ratio only as an internal helper to pick the penalty,\n",
    "            # and do NOT pass l1_ratio to LogisticRegression (to avoid warnings).\n",
    "            l1_ratio = trial.suggest_categorical(\"liblinear_l1_ratio\", [0.0, 1.0])\n",
    "            penalty = \"l1\" if l1_ratio == 1.0 else \"l2\"\n",
    "            params[\"penalty\"] = penalty\n",
    "            params.pop(\"l1_ratio\", None)\n",
    "            return params, sampling_strategy\n",
    "\n",
    "        if solver == \"saga\":\n",
    "            # saga supports L1, L2 and Elastic-Net. We let l1_ratio drive the effective penalty:\n",
    "            #   l1_ratio = 0   -> L2\n",
    "            #   l1_ratio = 1   -> L1\n",
    "            #   0 < l1_ratio < 1 -> Elastic-Net (only case where we pass l1_ratio)\n",
    "            l1_ratio = trial.suggest_float(\"saga_l1_ratio\", 0.0, 1.0)\n",
    "            if 0.0 < l1_ratio < 1.0:\n",
    "                params[\"penalty\"] = \"elasticnet\"\n",
    "                params[\"l1_ratio\"] = l1_ratio\n",
    "            elif l1_ratio <= 0.0:\n",
    "                params[\"penalty\"] = \"l2\"\n",
    "                params.pop(\"l1_ratio\", None)\n",
    "            else:\n",
    "                params[\"penalty\"] = \"l1\"\n",
    "                params.pop(\"l1_ratio\", None)\n",
    "            return params, sampling_strategy\n",
    "\n",
    "        # This point should not be reached, but keep a safe fallback\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = dict(base_params)\n",
    "\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 400)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 100)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 50)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.5])\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"])\n",
    "        criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "\n",
    "        params.update(\n",
    "            {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"min_samples_split\": min_samples_split,\n",
    "                \"min_samples_leaf\": min_samples_leaf,\n",
    "                \"max_features\": max_features,\n",
    "                \"class_weight\": class_weight,\n",
    "                \"criterion\": criterion,\n",
    "            }\n",
    "        )\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"GradientBoosting\":\n",
    "        params = dict(base_params)\n",
    "\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 5e-1, log=True)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 100)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.25, 1.0)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 50)\n",
    "        min_weight_fraction_leaf = trial.suggest_float(\"min_weight_fraction_leaf\", 0.0, 0.5)\n",
    "        min_impurity_decrease = trial.suggest_float(\"min_impurity_decrease\", 0.0, 0.5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.5])\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 50)\n",
    "\n",
    "        params.update(\n",
    "            {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"subsample\": subsample,\n",
    "                \"min_samples_leaf\": min_samples_leaf,\n",
    "                \"min_weight_fraction_leaf\": min_weight_fraction_leaf,\n",
    "                \"min_impurity_decrease\": min_impurity_decrease,\n",
    "                \"max_features\": max_features,\n",
    "                \"min_samples_split\": min_samples_split,\n",
    "            }\n",
    "        )\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"SVC\":\n",
    "        params = dict(base_params)\n",
    "\n",
    "        c_param = trial.suggest_float(\"C\", 1e-2, 1e2, log=True)\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "        tol = trial.suggest_float(\"tol\", 1e-4, 1e-1, log=True)\n",
    "        max_iter = trial.suggest_int(\"max_iter\", 1000, 10000)\n",
    "\n",
    "        params.update(\n",
    "            {\n",
    "                \"C\": c_param,\n",
    "                \"gamma\": gamma,\n",
    "                \"class_weight\": class_weight,\n",
    "                \"tol\": tol,\n",
    "                \"max_iter\": max_iter,\n",
    "            }\n",
    "        )\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"MLPClassifier\":\n",
    "        params = dict(base_params)\n",
    "\n",
    "        # Solver selection - adam is generally better for larger datasets\n",
    "        # solver = trial.suggest_categorical(\"solver\", [\"adam\", \"lbfgs\"])\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"adam\"])\n",
    "\n",
    "        # Keep the search space compact for runtime\n",
    "        # Use string labels for Optuna's categorical choices, then map to tuples\n",
    "        # to avoid warnings about non-primitive types in persistent storage.\n",
    "        hidden_layer_key = trial.suggest_categorical(\n",
    "            \"hidden_layer_sizes\",\n",
    "            # [\"64\", \"64-32\", \"128-64\", \"128-64-32\", \"256-128\"],\n",
    "            [\"64\", \"64-32\"],\n",
    "        )\n",
    "        hidden_layer_sizes = tuple(map(int, hidden_layer_key.split(\"-\")))\n",
    "\n",
    "        activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\", \"logistic\"])\n",
    "        alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-2, log=True)\n",
    "        max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "        tol = trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "        params.update(\n",
    "            {\n",
    "                \"solver\": solver,\n",
    "                \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "                \"activation\": activation,\n",
    "                \"alpha\": alpha,\n",
    "                \"max_iter\": max_iter,\n",
    "                \"tol\": tol,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Adam-specific parameters\n",
    "        if solver == \"adam\":\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-2, log=True)\n",
    "            beta_1 = trial.suggest_float(\"beta_1\", 0.90, 0.99)\n",
    "            beta_2 = trial.suggest_float(\"beta_2\", 0.99, 0.9999)\n",
    "            params.update(\n",
    "                {\n",
    "                    \"learning_rate_init\": learning_rate_init,\n",
    "                    \"beta_1\": beta_1,\n",
    "                    \"beta_2\": beta_2,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"LightGBM\":\n",
    "        params = dict(base_params)\n",
    "\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 5e-1, log=True)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 100)\n",
    "        num_leaves = trial.suggest_int(\"num_leaves\", 10, 200)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "        reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True)\n",
    "        reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True)\n",
    "        min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "        min_split_gain = trial.suggest_float(\"min_split_gain\", 0.0, 1.0)\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "        params.update(\n",
    "            {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"num_leaves\": num_leaves,\n",
    "                \"subsample\": subsample,\n",
    "                \"colsample_bytree\": colsample_bytree,\n",
    "                \"reg_alpha\": reg_alpha,\n",
    "                \"reg_lambda\": reg_lambda,\n",
    "                \"min_child_samples\": min_child_samples,\n",
    "                \"min_split_gain\": min_split_gain,\n",
    "                \"class_weight\": class_weight,\n",
    "            }\n",
    "        )\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    raise ValueError(f\"No search space defined for model_name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e282d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 10. Metrics: Configurable objectives and comprehensive evaluation\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand why metric choice matters for imbalanced data\n",
    "- Learn the strengths of MCC, F1, balanced accuracy, and cost-benefit\n",
    "- Configure objective functions for different application contexts\n",
    "- Distinguish between tuning objectives and evaluation metrics\n",
    "\n",
    "We support multiple tuning objectives (configured via `TUNING_OBJECTIVE`):\n",
    "- **[Matthews correlation coefficient (MCC)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)**: Robust to class imbalance, accounts for all confusion matrix elements\n",
    "- **[F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**: Harmonic mean of precision and recall\n",
    "- **[Balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)**: Average of sensitivity and specificity\n",
    "- **Cost-benefit**: Cost-driven metric using configurable cost matrix\n",
    "\n",
    "Additionally, we evaluate models using: **accuracy**, **precision**, **recall**, **specificity**, **ROC AUC**, **PR AUC**.\n",
    "\n",
    "**Why MCC?**\n",
    "- Single-number summary handling imbalance well\n",
    "- Takes into account all four cells of the confusion matrix (TP, TN, FP, FN)\n",
    "- Ranges from -1 (total disagreement) through 0 (random) to +1 (perfect)\n",
    "- More informative than accuracy for imbalanced datasets\n",
    "\n",
    "**Common Pitfall:** Don't rely solely on accuracy for imbalanced data. A model predicting all majority class gets high accuracy but zero usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17865c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCC scorer for Optuna (higher is better)\n",
    "# Note: specificity_score is defined earlier in Section 4\n",
    "mcc_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)\n",
    "\n",
    "\n",
    "# Cost-benefit scorer\n",
    "def cost_benefit_scorer_func(y_true, y_pred):\n",
    "    \"\"\"Wrapper for cost_benefit_score using the global COST_MATRIX.\n",
    "\n",
    "    Uses normalize=False for scoring to preserve relative differences\n",
    "    for optimization. Normalized scores are computed for display.\n",
    "    \"\"\"\n",
    "    return cost_benefit_score(y_true, y_pred, COST_MATRIX, normalize=False)\n",
    "\n",
    "\n",
    "cost_benefit_scorer = make_scorer(cost_benefit_scorer_func, greater_is_better=True)\n",
    "\n",
    "# Determine which scorer to use based on TUNING_OBJECTIVE\n",
    "if TUNING_OBJECTIVE == \"mcc\":\n",
    "    tuning_scorer = mcc_scorer\n",
    "    tuning_scorer_name = \"MCC\"\n",
    "elif TUNING_OBJECTIVE == \"f1\":\n",
    "    tuning_scorer = make_scorer(f1_score, greater_is_better=True)\n",
    "    tuning_scorer_name = \"F1\"\n",
    "elif TUNING_OBJECTIVE == \"balanced_accuracy\":\n",
    "    tuning_scorer = make_scorer(balanced_accuracy_score, greater_is_better=True)\n",
    "    tuning_scorer_name = \"Balanced Accuracy\"\n",
    "elif TUNING_OBJECTIVE == \"cost_benefit\":\n",
    "    tuning_scorer = cost_benefit_scorer\n",
    "    tuning_scorer_name = \"Cost-Benefit\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown TUNING_OBJECTIVE: {TUNING_OBJECTIVE}\")\n",
    "\n",
    "print(f\"Optimizing for: {tuning_scorer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836dc54",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter tuning with Optuna\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand Bayesian optimization vs grid/random search\n",
    "- Configure and run Optuna studies\n",
    "- Integrate sampling strategies into hyperparameter tuning\n",
    "- Interpret Optuna trial results and convergence\n",
    "\n",
    "### Comparison of Hyperparameter Search Methods\n",
    "\n",
    "| Method | Strategy | Pros | Cons | Best For |\n",
    "|--------|----------|------|------|----------|\n",
    "| **[Grid Search](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.GridSampler.html)** | Exhaustive search over specified parameter grid | Simple, reproducible, guaranteed to find best in grid | Computationally expensive, curse of dimensionality, misses values between grid points | Small search spaces, few parameters |\n",
    "| **[Random Search](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.RandomSampler.html)** | Random sampling from parameter distributions | More efficient than grid for high-dimensional spaces, can find good solutions quickly | No learning between trials, may miss optimal regions | Moderate search spaces, initial exploration |\n",
    "| **[Bayesian (TPE)](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html)** | Probabilistic model guides search towards promising regions | Efficient, learns from previous trials, handles complex spaces well | More complex setup, overhead for small searches | Large search spaces, expensive evaluations |\n",
    "\n",
    "**[Optuna](https://optuna.org/)** is a hyperparameter optimization framework using [Tree-structured Parzen Estimator (TPE)](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html) for efficient Bayesian search. Advantages over grid/random search:\n",
    "\n",
    "1. **Adaptive sampling**: Learns from previous trials to suggest promising regions\n",
    "2. **Early stopping**: Prunes unpromising trials (with pruners) - Though this only works if the model supports partial fitting (most sklearn models do not)\n",
    "3. **Flexible search spaces**: Continuous, discrete, categorical parameters\n",
    "4. **Parallel execution**: Multiple workers can run trials simultaneously\n",
    "\n",
    "We define an **objective function** that:\n",
    "- Samples hyperparameters from the search space\n",
    "- Builds a pipeline with sampled hyperparameters\n",
    "- Evaluates using cross-validation\n",
    "- Returns the metric to optimize\n",
    "\n",
    "Optuna maximizes the objective by default (we use `greater_is_better=True` for all our metrics).\n",
    "\n",
    "**Common Pitfall:** Running too few trials may miss the optimal region. Start with at least 50-100 trials per model (more for complex spaces).\n",
    "\n",
    "We now set up [Optuna](https://optuna.org/) studies for the selected models in `TUNED_MODELS`.\n",
    "- Each trial samples hyperparameters from `suggest_params_for_model`.\n",
    "- We build a full preprocessing + model [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "- We evaluate using [`RepeatedStratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html) with `N_SPLITS` and `N_REPEATS_TUNING`.\n",
    "- The objective is the **mean `TUNING_OBJECTIVE`** across all folds and repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress optuna logging for cleaner output\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "def make_tuning_cv():\n",
    "    return RepeatedStratifiedKFold(\n",
    "        n_splits=N_SPLITS,\n",
    "        n_repeats=N_REPEATS_TUNING,\n",
    "        random_state=SEED_TUNING,\n",
    "    )\n",
    "\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    \"\"\"Optuna objective function for hyperparameter tuning.\n",
    "\n",
    "    Tunes both model hyperparameters and sampling strategy, evaluating using\n",
    "    the configured TUNING_OBJECTIVE metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object.\n",
    "    model_name : str\n",
    "        Name of the model to tune.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean score across all CV folds for the tuning objective.\n",
    "    \"\"\"\n",
    "    params, sampling_strategy = suggest_params_for_model(trial, model_name)\n",
    "    estimator = create_base_estimator(model_name, params=params)\n",
    "    pipe = build_pipeline(estimator, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    cv = make_tuning_cv()\n",
    "    with warnings.catch_warnings():\n",
    "        # Suppress ConvergenceWarning (common for LogisticRegression during tuning)\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'\",\n",
    "            category=UserWarning,\n",
    "            module=\"sklearn.linear_model._logistic\",\n",
    "        )\n",
    "        scores = cross_val_score(\n",
    "            pipe,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=tuning_scorer,\n",
    "            n_jobs=NUM_JOBS,\n",
    "        )\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "tuning_records = []\n",
    "best_params_by_model = {}\n",
    "studies_by_model = {}  # Store studies for later analysis\n",
    "\n",
    "for model_name in TUNED_MODELS:\n",
    "    print(f\"Starting Optuna tuning for: {model_name}\")\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler, study_name=f\"{model_name}_{TUNING_OBJECTIVE}_tuning\")\n",
    "\n",
    "    def _objective(trial):\n",
    "        \"\"\"Wrapper as optuna only expects a single argument for the objective function.\"\"\"\n",
    "        return objective(trial, model_name)\n",
    "\n",
    "    # Get number of trials for this model type, else set to default\n",
    "    N_TRIALS = N_TRIALS_PER_MODEL.get(model_name, N_TRIALS_PER_MODEL_DEFAULT)\n",
    "    study.optimize(_objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best {tuning_scorer_name} for {model_name}: {study.best_value:.4f}\")\n",
    "    print(\"Best params:\", study.best_params)\n",
    "\n",
    "    best_params_by_model[model_name] = study.best_params\n",
    "    studies_by_model[model_name] = study\n",
    "\n",
    "    for trial in study.trials:\n",
    "        rec = {\n",
    "            \"model\": model_name,\n",
    "            \"trial\": trial.number,\n",
    "            f\"objective_{TUNING_OBJECTIVE}\": trial.value,\n",
    "            \"n_splits\": N_SPLITS,\n",
    "            \"n_repeats_tuning\": N_REPEATS_TUNING,\n",
    "        }\n",
    "        for k, v in trial.params.items():\n",
    "            rec[f\"param_{k}\"] = v\n",
    "        tuning_records.append(rec)\n",
    "\n",
    "tuning_results_df = pd.DataFrame(tuning_records) if tuning_records else pd.DataFrame()\n",
    "\n",
    "# Save tuning results to tables folder\n",
    "if not tuning_results_df.empty:\n",
    "    tuning_csv_path = os.path.join(tables_folder, \"optuna_tuning_all_trials.csv\")\n",
    "    tuning_results_df.to_csv(tuning_csv_path, index=False)\n",
    "    print(f\"Saved tuning results to: {tuning_csv_path}\")\n",
    "\n",
    "# Save best params by model to CSV\n",
    "if best_params_by_model:\n",
    "    best_params_records = []\n",
    "    for model_name, params in best_params_by_model.items():\n",
    "        rec = {\"model\": model_name}\n",
    "        rec.update(params)\n",
    "        best_params_records.append(rec)\n",
    "    best_params_df = pd.DataFrame(best_params_records)\n",
    "    best_params_csv_path = os.path.join(tables_folder, \"best_params_by_model.csv\")\n",
    "    best_params_df.to_csv(best_params_csv_path, index=False)\n",
    "    print(f\"Saved best params by model to: {best_params_csv_path}\")\n",
    "\n",
    "tuning_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289360cb",
   "metadata": {},
   "source": [
    "## 12. Hyperparameter Importance Analysis\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand which hyperparameters have the greatest impact on model performance\n",
    "- Learn to interpret hyperparameter importance plots\n",
    "- Use importance analysis to focus future tuning efforts\n",
    "\n",
    "After hyperparameter tuning, it's valuable to understand which hyperparameters had the greatest impact on the objective metric. Optuna provides tools to compute parameter importance using [functional ANOVA (fANOVA)](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.FanovaImportanceEvaluator.html#optuna.importance.FanovaImportanceEvaluator).\n",
    "\n",
    "**Parameter Importance** measures how much each hyperparameter contributes to variance in the objective function. High importance means the parameter has a strong effect on performance.\n",
    "\n",
    "**Use Cases:**\n",
    "- **Focus tuning efforts**: Spend more trials on important parameters\n",
    "- **Model understanding**: Learn what drives your model's performance\n",
    "- **Feature engineering**: Important preprocessing params may suggest data issues\n",
    "\n",
    "We'll generate:\n",
    "1. [Hyperparameter importance bar plots](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_param_importances.html) (saved to optuna folder)\n",
    "2. [Parallel coordinate plots](https://optuna.readthedocs.io/en/stable/reference/visualization/matplotlib/generated/optuna.visualization.matplotlib.parallel_coordinate.html#) showing parameter interactions\n",
    "3. CSV files with importance values for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48002b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize hyperparameter importance for each tuned model\n",
    "for model_name, study in studies_by_model.items():\n",
    "    print(f\"\\nAnalyzing hyperparameter importance for: {model_name}\")\n",
    "\n",
    "    # Compute importance\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "\n",
    "        # Create importance dataframe and save to CSV\n",
    "        importance_df = pd.DataFrame(\n",
    "            {\"parameter\": list(importance.keys()), \"importance\": list(importance.values())}\n",
    "        ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "        importance_csv_path = os.path.join(optuna_folder, f\"{model_name}_param_importance.csv\")\n",
    "        importance_df.to_csv(importance_csv_path, index=False)\n",
    "        print(f\"  Saved importance to: {importance_csv_path}\")\n",
    "        print(f\"  Top 3 parameters: {', '.join(importance_df.head(3)['parameter'].tolist())}\")\n",
    "\n",
    "        # Plot importance\n",
    "        fig = plot_param_importances(study)\n",
    "        fig.update_layout(title=f\"Hyperparameter Importance: {model_name}\")\n",
    "        importance_plot_path = os.path.join(optuna_folder, f\"{model_name}_param_importance.html\")\n",
    "        fig.write_html(importance_plot_path)\n",
    "        print(f\"  Saved importance plot to: {importance_plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not compute importance for {model_name}: {e}\")\n",
    "\n",
    "    # Generate parallel coordinate plot\n",
    "    try:\n",
    "        fig = plot_parallel_coordinate(study, params=None)\n",
    "        fig.update_layout(title=f\"Parallel Coordinate Plot: {model_name}\")\n",
    "        parallel_plot_path = os.path.join(optuna_folder, f\"{model_name}_parallel_coordinate.html\")\n",
    "        fig.write_html(parallel_plot_path)\n",
    "        print(f\"  Saved parallel coordinate plot to: {parallel_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not create parallel coordinate plot for {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\nAll hyperparameter analysis artifacts saved to: {optuna_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1170018",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 13. Post-tuning cross-validation with many metrics\n",
    "\n",
    "### Learning Objectives:\n",
    "- Perform comprehensive model evaluation across multiple metrics\n",
    "- Understand the difference between tuning and final evaluation\n",
    "- Interpret metric trade-offs (precision vs recall, etc.)\n",
    "- Select the best model based on primary objective\n",
    "\n",
    "After hyperparameter tuning, we evaluate all models using **stratified K-fold cross-validation** with multiple metrics:\n",
    "\n",
    "- **MCC**: Primary metric for imbalanced binary classification\n",
    "- **Accuracy**: Overall correctness (can be misleading for imbalanced data)\n",
    "- **Precision**: Of predicted positives, how many are truly positive\n",
    "- **Recall (Sensitivity)**: Of actual positives, how many are detected\n",
    "- **F1**: Harmonic mean of precision and recall\n",
    "- **Balanced Accuracy**: Average of sensitivity and specificity\n",
    "- **Specificity**: Of actual negatives, how many are correctly identified\n",
    "- **ROC AUC**: Ranking performance across all thresholds\n",
    "- **PR AUC**: Precision-recall trade-off (preferred for imbalanced data)\n",
    "- **Cost-Benefit**: Cost-driven metric\n",
    "\n",
    "We now evaluate (potentially tuned) models using a richer set of metrics over:\n",
    "- `N_SPLITS` folds,\n",
    "- `N_REPEATS_CV` repeats,\n",
    "using [`RepeatedStratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html).\n",
    "\n",
    "For each model, fold and repeat we record:\n",
    "- MCC, accuracy, precision, recall, F1, balanced accuracy, specificity, ROC AUC, PR AUC.\n",
    "These are stored in a long-form `cv_results_df` for detailed analysis, and then summarised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_cv():\n",
    "    return RepeatedStratifiedKFold(\n",
    "        n_splits=N_SPLITS,\n",
    "        n_repeats=N_REPEATS_CV,\n",
    "        random_state=SEED_CV,\n",
    "    )\n",
    "\n",
    "\n",
    "EVAL_MODELS = MODEL_NAMES  # evaluate all models; some may use default params\n",
    "\n",
    "cv_records = []\n",
    "rskf = make_eval_cv()\n",
    "\n",
    "for model_name in EVAL_MODELS:\n",
    "    # Get best params, extracting sampling_strategy if present\n",
    "    # Use dict() to copy so we don't mutate the stored params\n",
    "    all_params = dict(best_params_by_model.get(model_name, {}))\n",
    "    sampling_strategy = all_params.pop(\"sampling_strategy\", \"none\")\n",
    "    params = all_params\n",
    "\n",
    "    estimator = create_base_estimator(model_name, params=params)\n",
    "    pipe = build_pipeline(estimator, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    print(f\"Evaluating model (CV): {model_name}\")\n",
    "    for split_idx, (train_idx, val_idx) in enumerate(rskf.split(X_train, y_train), start=1):\n",
    "        print(f\"\\tRepeat number : {(split_idx - 1) // N_SPLITS + 1}, Fold number: {(split_idx - 1) % N_SPLITS + 1}\")\n",
    "        repeat_idx = (split_idx - 1) // N_SPLITS + 1\n",
    "        fold_idx = (split_idx - 1) % N_SPLITS + 1\n",
    "\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'\",\n",
    "                category=UserWarning,\n",
    "                module=\"sklearn.linear_model._logistic\",\n",
    "            )\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            y_val_pred = pipe.predict(X_val)\n",
    "\n",
    "        # Many metrics rely on probabilities; assume binary classification and positive class = 1\n",
    "        if hasattr(pipe, \"predict_proba\"):\n",
    "            y_val_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            # Fallback: use decision_function if available, otherwise cast predictions to {0,1}\n",
    "            if hasattr(pipe, \"decision_function\"):\n",
    "                scores = pipe.decision_function(X_val)\n",
    "                # Map scores to [0, 1] via rank-based scaling for AUC-like metrics\n",
    "                ranks = pd.Series(scores).rank(method=\"average\").values\n",
    "                y_val_proba = ranks / ranks.max()\n",
    "            else:\n",
    "                y_val_proba = y_val_pred.astype(float)\n",
    "\n",
    "        mcc = matthews_corrcoef(y_val, y_val_pred)\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        prec = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "        rec = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "        bal_acc = balanced_accuracy_score(y_val, y_val_pred)\n",
    "        spec = specificity_score(y_val, y_val_pred)\n",
    "        cost_ben = cost_benefit_score(y_val, y_val_pred, COST_MATRIX, normalize=False)\n",
    "        cost_ben_norm = cost_benefit_score(y_val, y_val_pred, COST_MATRIX, normalize=True)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        except ValueError:\n",
    "            roc_auc = np.nan\n",
    "        try:\n",
    "            pr_auc = average_precision_score(y_val, y_val_proba)\n",
    "        except ValueError:\n",
    "            pr_auc = np.nan\n",
    "\n",
    "        print(\n",
    "            f\"\\t\\tMCC: {mcc:.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}, Bal_Acc: {bal_acc:.4f}, Spec: {spec:.4f}, ROC_AUC: {roc_auc:.4f}, PR_AUC: {pr_auc:.4f}, Cost-Benefit: {cost_ben_norm:.4f}\"\n",
    "        )\n",
    "\n",
    "        cv_records.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"repeat\": repeat_idx,\n",
    "                \"fold\": fold_idx,\n",
    "                \"n_splits\": N_SPLITS,\n",
    "                \"n_repeats_cv\": N_REPEATS_CV,\n",
    "                \"sampling_strategy\": sampling_strategy,\n",
    "                \"mcc\": mcc,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"f1\": f1,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"specificity\": spec,\n",
    "                \"cost_benefit_raw\": cost_ben,\n",
    "                \"cost_benefit\": cost_ben_norm,\n",
    "                \"roc_auc\": roc_auc,\n",
    "                \"pr_auc\": pr_auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_records)\n",
    "\n",
    "# Save CV results to tables folder\n",
    "cv_results_csv_path = os.path.join(tables_folder, \"cv_results_per_fold.csv\")\n",
    "cv_results_df.to_csv(cv_results_csv_path, index=False)\n",
    "print(f\"Saved CV results to: {cv_results_csv_path}\")\n",
    "\n",
    "cv_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5e641",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Summary: mean and standard deviation of metrics per model\n",
    "metric_cols = [\n",
    "    \"mcc\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"f1\",\n",
    "    \"balanced_accuracy\",\n",
    "    \"specificity\",\n",
    "    \"cost_benefit\",\n",
    "    \"roc_auc\",\n",
    "    \"pr_auc\",\n",
    "]\n",
    "\n",
    "\n",
    "# Save CV results to tables folder\n",
    "cv_summary_df = cv_results_df.groupby(\"model\")[metric_cols].agg([\"mean\", \"std\"])\n",
    "cv_summary_df\n",
    "\n",
    "\n",
    "# Flatten the MultiIndex columns: ('mcc', 'mean') -> 'mean_mcc'\n",
    "model_comparison_df = cv_summary_df.copy()\n",
    "model_comparison_df.columns = [f\"{stat}_{metric}\" for metric, stat in model_comparison_df.columns]\n",
    "model_comparison_df = model_comparison_df.reset_index()\n",
    "\n",
    "# Save model comparison to tables folder\n",
    "model_comparison_csv_path = os.path.join(tables_folder, \"model_comparison_summary.csv\")\n",
    "model_comparison_df.sort_values(f\"mean_{TUNING_OBJECTIVE}\", ascending=False, inplace=True)\n",
    "model_comparison_df.to_csv(model_comparison_csv_path, index=False)\n",
    "print(f\"Saved model comparison summary to: {model_comparison_csv_path}\")\n",
    "model_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a491fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flattened comparison dataframe for easier access\n",
    "model_comparison_df = pd.DataFrame()\n",
    "for metric in metric_cols:\n",
    "    model_comparison_df[f\"mean_{metric}\"] = cv_results_df.groupby(\"model\")[metric].mean()\n",
    "    model_comparison_df[f\"std_{metric}\"] = cv_results_df.groupby(\"model\")[metric].std()\n",
    "\n",
    "model_comparison_df = model_comparison_df.reset_index()\n",
    "\n",
    "# Save model comparison to tables folder\n",
    "model_comparison_csv_path = os.path.join(tables_folder, \"model_comparison_summary.csv\")\n",
    "model_comparison_df.to_csv(model_comparison_csv_path, index=False)\n",
    "print(f\"Saved model comparison summary to: {model_comparison_csv_path}\")\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "model_comparison_df.sort_values(\"mean_mcc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b47089",
   "metadata": {},
   "source": [
    "## 14. Select the best model by primary objective and refit on full training set\n",
    "\n",
    "### Learning Objectives:\n",
    "- Apply model selection criteria consistently\n",
    "- Understand final model training on full dataset\n",
    "- Recognize when to refit vs when to save CV models\n",
    "- Prepare for final evaluation on held-out test set\n",
    "\n",
    "We select the best model based on mean cross-validation performance for our tuning objective (configured via `TUNING_OBJECTIVE`). The selection process:\n",
    "\n",
    "1. **Rank models** by mean CV score on the tuning objective\n",
    "2. **Select the top model** (highest mean score)\n",
    "3. **Refit on full training set** using optimal hyperparameters\n",
    "4. **Prepare for test evaluation** (done only once to avoid overfitting)\n",
    "\n",
    "**Why refit on full training data?**\n",
    "- CV uses k-1 folds for training; we can improve performance by using all training data\n",
    "- Ensures the final model has maximum information for deployment\n",
    "- Test set evaluation remains unbiased (test set never seen during training/tuning)\n",
    "\n",
    "**Common Pitfall:** Selecting models based on test set performance invalidates the evaluation. Always select on validation/CV, then evaluate once on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on tuning objective\n",
    "# Map TUNING_OBJECTIVE to the corresponding column in cv_results_df\n",
    "# All map exactly\n",
    "objective_col_map = {\n",
    "    \"mcc\": \"mcc\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "    \"cost_benefit\": \"cost_benefit\",\n",
    "}\n",
    "objective_col = objective_col_map.get(TUNING_OBJECTIVE, \"mcc\")\n",
    "\n",
    "mean_score_by_model = cv_results_df.groupby(\"model\")[objective_col].mean().sort_values(ascending=False)\n",
    "best_model_name = mean_score_by_model.index[0]\n",
    "print(f\"Mean {TUNING_OBJECTIVE} by model:\")\n",
    "print(mean_score_by_model)\n",
    "print()\n",
    "print(f\"Best model by {TUNING_OBJECTIVE}: {best_model_name}\")\n",
    "\n",
    "best_params = best_params_by_model.get(best_model_name, {})\n",
    "print(\"Best params used for this model (if tuned):\", best_params)\n",
    "\n",
    "# Extract sampling_strategy if present\n",
    "all_params = best_params.copy()\n",
    "sampling_strategy = all_params.pop(\"sampling_strategy\", \"none\")\n",
    "\n",
    "best_estimator = create_base_estimator(best_model_name, params=all_params)\n",
    "best_pipeline = build_pipeline(best_estimator, sampling_strategy=sampling_strategy)\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "print(f\"Fitted best model pipeline (sampling: {sampling_strategy})\")\n",
    "\n",
    "# Store best_name for later use\n",
    "best_name = best_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7d79b",
   "metadata": {},
   "source": [
    "## 15. Final evaluation on the held-out test set\n",
    "\n",
    "### Learning Objectives:\n",
    "- Perform unbiased final evaluation on test data\n",
    "- Generate comprehensive diagnostic visualizations\n",
    "- Interpret confusion matrix and ROC/PR curves\n",
    "- Understand the importance of single-use test set\n",
    "\n",
    "**The held-out test set provides an unbiased estimate of real-world performance.** This evaluation happens only once to avoid overfitting to test data.\n",
    "\n",
    "We evaluate the best model using:\n",
    "- **Confusion matrix**: Visualize TP, TN, FP, FN\n",
    "- **All metrics**: MCC, accuracy, precision, recall, F1, balanced accuracy, specificity, ROC AUC, PR AUC, cost-benefit\n",
    "- **ROC curve**: Trade-off between sensitivity and specificity across thresholds\n",
    "- **Precision-Recall curve**: More informative than ROC for imbalanced data\n",
    "\n",
    "**Common Pitfall:** Evaluating on the test set multiple times (e.g., after tweaking hyperparameters) causes overfitting to test data. Test once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffada43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "y_test_proba = best_pipeline.predict_proba(X_test)[:, 1]  # prediction class [<=50,>50], positive class column 1\n",
    "\n",
    "test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "test_rec = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "test_spec = specificity_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_pr_auc = average_precision_score(y_test, y_test_proba)\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test MCC: {test_mcc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test precision: {test_prec:.4f}\")\n",
    "print(f\"Test recall (sensitivity): {test_rec:.4f}\")\n",
    "print(f\"Test F1: {test_f1:.4f}\")\n",
    "print(f\"Test balanced accuracy: {test_bal_acc:.4f}\")\n",
    "print(f\"Test specificity: {test_spec:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"Test PR AUC (average precision): {test_pr_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lb.classes_)\n",
    "disp.plot(cmap=\"Reds\", values_format=\"d\")\n",
    "ax = plt.gca()\n",
    "ax.grid(False)\n",
    "plt.title(f\"Confusion matrix on held-out test set ({best_model_name})\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(out_folder, f\"confusion_matrix_on_held_out_test_set_{out_suffix}.pdf\")\n",
    "plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Saved confusion matrix figure to: {out_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "roc_auc_val = auc(fpr, tpr)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc_val:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "plt.title(\"ROC curve (test set)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "out_path = os.path.join(out_folder, f\"roc_curve_test_best_model_{out_suffix}.pdf\")\n",
    "plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Saved ROC curve figure to: {out_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-recall curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "pr_auc_val = average_precision_score(y_test, y_test_proba)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(rec, prec, label=f\"PR curve (AP = {pr_auc_val:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curve (test set)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "out_path = os.path.join(out_folder, f\"pr_curve_test_best_model_{out_suffix}.pdf\")\n",
    "plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Saved PR curve figure to: {out_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5460805",
   "metadata": {},
   "source": [
    "## 16. Threshold Analysis and Optimization\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand why the default 0.5 threshold may not be optimal\n",
    "- Learn how to find optimal thresholds for different objectives\n",
    "- Apply threshold optimization to improve model performance\n",
    "- Understand the trade-offs between different thresholds\n",
    "\n",
    "The default classification threshold of 0.5 assumes:\n",
    "1. Equal costs for false positives and false negatives\n",
    "2. Balanced class distribution\n",
    "3. Well-calibrated probabilities\n",
    "\n",
    "In practice, these assumptions rarely hold. By optimizing the threshold, we can:\n",
    "- Maximize a specific metric (MCC, F1, etc.)\n",
    "- Minimize misclassification costs (using cost-benefit analysis)\n",
    "- Balance precision and recall according to application needs\n",
    "\n",
    "**Common Pitfall:** Optimizing threshold on the test set causes overfitting. Always use validation data or cross-validation to find the optimal threshold, then evaluate once on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc368e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.0, 1.0, 101)\n",
    "th_records = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    # Calculate +ve predictions at each threshold level - y_thr\n",
    "    y_thr = (y_test_proba >= thr).astype(int)\n",
    "    # Compare predictions vs truth (y_test)\n",
    "    mcc_thr = matthews_corrcoef(y_test, y_thr)\n",
    "    acc_thr = accuracy_score(y_test, y_thr)\n",
    "    prec_thr = precision_score(y_test, y_thr, zero_division=0)\n",
    "    rec_thr = recall_score(y_test, y_thr, zero_division=0)\n",
    "    f1_thr = f1_score(y_test, y_thr, zero_division=0)\n",
    "    bal_acc_thr = balanced_accuracy_score(y_test, y_thr)\n",
    "    spec_thr = specificity_score(y_test, y_thr)\n",
    "    cost_ben_raw = cost_benefit_score(y_test, y_thr, COST_MATRIX, normalize=False)\n",
    "    cost_ben_norm = cost_benefit_score(y_test, y_thr, COST_MATRIX, normalize=True)\n",
    "    th_records.append(\n",
    "        {\n",
    "            \"threshold\": thr,\n",
    "            \"mcc\": mcc_thr,\n",
    "            \"accuracy\": acc_thr,\n",
    "            \"precision\": prec_thr,\n",
    "            \"recall\": rec_thr,\n",
    "            \"f1\": f1_thr,\n",
    "            \"balanced_accuracy\": bal_acc_thr,\n",
    "            \"specificity\": spec_thr,\n",
    "            \"cost_benefit_raw\": cost_ben_raw,\n",
    "            \"cost_benefit\": cost_ben_norm,\n",
    "        }\n",
    "    )\n",
    "\n",
    "threshold_df = pd.DataFrame(th_records)\n",
    "\n",
    "# Save threshold analysis to tables folder\n",
    "threshold_csv_path = os.path.join(tables_folder, \"threshold_analysis.csv\")\n",
    "threshold_df.to_csv(threshold_csv_path, index=False)\n",
    "print(f\"Saved threshold analysis to: {threshold_csv_path}\")\n",
    "\n",
    "threshold_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b575b48",
   "metadata": {},
   "source": [
    "### Finding Optimal Thresholds\n",
    "\n",
    "We identify the optimal threshold for each metric. Note that different metrics may prefer different thresholds:\n",
    "- **MCC**: Balances all confusion matrix elements\n",
    "- **F1**: Balances precision and recall\n",
    "- **Balanced Accuracy**: Balances sensitivity and specificity\n",
    "- **Cost-Benefit**: Minimizes misclassification costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal thresholds for different objectives\n",
    "optimal_thresholds = {}\n",
    "\n",
    "for metric in [\"mcc\", \"f1\", \"balanced_accuracy\", \"cost_benefit\"]:\n",
    "    if metric == \"cost_benefit\":\n",
    "        # For cost_benefit, we want to maximize (most positive value)\n",
    "        optimal_idx = threshold_df[metric].idxmax()\n",
    "    else:\n",
    "        optimal_idx = threshold_df[metric].idxmax()\n",
    "\n",
    "    optimal_threshold = threshold_df.loc[optimal_idx, \"threshold\"]\n",
    "    optimal_value = threshold_df.loc[optimal_idx, metric]\n",
    "    optimal_thresholds[metric] = optimal_threshold\n",
    "\n",
    "    print(f\"Optimal threshold for {metric}: {optimal_threshold:.3f} (value: {optimal_value:.4f})\")\n",
    "\n",
    "# Determine which threshold to use based on configuration\n",
    "if THRESHOLD_METRIC == \"auto\":\n",
    "    threshold_metric_to_use = TUNING_OBJECTIVE\n",
    "else:\n",
    "    threshold_metric_to_use = THRESHOLD_METRIC\n",
    "\n",
    "optimal_threshold_to_use = optimal_thresholds.get(threshold_metric_to_use, 0.5)\n",
    "print(f\"\\nUsing optimal threshold for {threshold_metric_to_use}: {optimal_threshold_to_use:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dbd53",
   "metadata": {},
   "source": [
    "### Threshold plots\n",
    "Visualize the chosen thresholds vs metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f938d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple metrics vs threshold\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: MCC, F1, Balanced Accuracy\n",
    "ax = axes[0, 0]\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"mcc\"], label=\"MCC\", linewidth=2)\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"f1\"], label=\"F1\", linewidth=2)\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"balanced_accuracy\"], label=\"Balanced Acc\", linewidth=2)\n",
    "ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Default (0.5)\")\n",
    "ax.axvline(\n",
    "    optimal_threshold_to_use, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Optimal ({optimal_threshold_to_use:.2f})\"\n",
    ")\n",
    "ax.set_xlabel(\"Decision Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Comprehensive Metrics vs Threshold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision and Recall trade-off\n",
    "ax = axes[0, 1]\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"precision\"], label=\"Precision\", linewidth=2, color=\"blue\")\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"recall\"], label=\"Recall\", linewidth=2, color=\"orange\")\n",
    "ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.axvline(optimal_threshold_to_use, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "ax.set_xlabel(\"Decision Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Precision-Recall Trade-off\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cost-Benefit\n",
    "ax = axes[1, 0]\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"cost_benefit\"], label=\"Cost-Benefit\", linewidth=2, color=\"green\")\n",
    "ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.axvline(optimal_threshold_to_use, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "ax.set_xlabel(\"Decision Threshold\")\n",
    "ax.set_ylabel(\"Cost-Benefit Score (Higher is Better)\")\n",
    "ax.set_title(\"Cost-Benefit Analysis\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Sensitivity (Recall) and Specificity\n",
    "ax = axes[1, 1]\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"recall\"], label=\"Sensitivity (Recall)\", linewidth=2, color=\"orange\")\n",
    "ax.plot(threshold_df[\"threshold\"], threshold_df[\"specificity\"], label=\"Specificity\", linewidth=2, color=\"purple\")\n",
    "ax.axvline(0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.axvline(optimal_threshold_to_use, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "ax.set_xlabel(\"Decision Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Sensitivity vs Specificity\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "out_path = os.path.join(out_folder, f\"threshold_analysis_comprehensive_{out_suffix}.pdf\")\n",
    "plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Saved comprehensive threshold analysis to: {out_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f5ac8",
   "metadata": {},
   "source": [
    "### Applying Optimal Threshold to Best Model\n",
    "\n",
    "If `USE_OPTIMAL_THRESHOLD` is enabled, we create a thresholded version of the best model that uses the optimal threshold instead of the default 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OPTIMAL_THRESHOLD:\n",
    "    print(f\"Creating thresholded classifier with optimal threshold: {optimal_threshold_to_use:.3f}\")\n",
    "    best_pipeline_thresholded = ThresholdedClassifier(best_pipeline, threshold=optimal_threshold_to_use)\n",
    "\n",
    "    # Re-evaluate with optimal threshold\n",
    "    y_test_pred_thresholded = best_pipeline_thresholded.predict(X_test)\n",
    "\n",
    "    # Compute metrics with optimal threshold\n",
    "    test_mcc_thresh = matthews_corrcoef(y_test, y_test_pred_thresholded)\n",
    "    test_acc_thresh = accuracy_score(y_test, y_test_pred_thresholded)\n",
    "    test_prec_thresh = precision_score(y_test, y_test_pred_thresholded, zero_division=0)\n",
    "    test_rec_thresh = recall_score(y_test, y_test_pred_thresholded, zero_division=0)\n",
    "    test_f1_thresh = f1_score(y_test, y_test_pred_thresholded, zero_division=0)\n",
    "    test_bal_acc_thresh = balanced_accuracy_score(y_test, y_test_pred_thresholded)\n",
    "    test_spec_thresh = specificity_score(y_test, y_test_pred_thresholded)\n",
    "    test_cost_ben_thresh_raw = cost_benefit_score(y_test, y_test_pred_thresholded, COST_MATRIX, normalize=False)\n",
    "    test_cost_ben_thresh = cost_benefit_score(y_test, y_test_pred_thresholded, COST_MATRIX, normalize=True)\n",
    "\n",
    "    # Compute default threshold cost-benefit for comparison\n",
    "    test_cost_ben_default_raw = cost_benefit_score(y_test, y_test_pred, COST_MATRIX, normalize=False)\n",
    "    test_cost_ben_default = cost_benefit_score(y_test, y_test_pred, COST_MATRIX, normalize=True)\n",
    "\n",
    "    print(\n",
    "        f\"\\n{'Metric':<20} {'Default (0.5)':<15} {'Optimal (' + f'{optimal_threshold_to_use:.2f}' + ')':<15} {'Improvement':<15}\"\n",
    "    )\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'MCC':<20} {test_mcc:<15.4f} {test_mcc_thresh:<15.4f} {test_mcc_thresh - test_mcc:+.4f}\")\n",
    "    print(f\"{'Accuracy':<20} {test_acc:<15.4f} {test_acc_thresh:<15.4f} {test_acc_thresh - test_acc:+.4f}\")\n",
    "    print(f\"{'Precision':<20} {test_prec:<15.4f} {test_prec_thresh:<15.4f} {test_prec_thresh - test_prec:+.4f}\")\n",
    "    print(f\"{'Recall':<20} {test_rec:<15.4f} {test_rec_thresh:<15.4f} {test_rec_thresh - test_rec:+.4f}\")\n",
    "    print(f\"{'F1':<20} {test_f1:<15.4f} {test_f1_thresh:<15.4f} {test_f1_thresh - test_f1:+.4f}\")\n",
    "    print(\n",
    "        f\"{'Balanced Acc':<20} {test_bal_acc:<15.4f} {test_bal_acc_thresh:<15.4f} {test_bal_acc_thresh - test_bal_acc:+.4f}\"\n",
    "    )\n",
    "    print(f\"{'Specificity':<20} {test_spec:<15.4f} {test_spec_thresh:<15.4f} {test_spec_thresh - test_spec:+.4f}\")\n",
    "    print(\n",
    "        f\"{'Cost-Benefit':<20} {test_cost_ben_default:<15.4f} {test_cost_ben_thresh:<15.4f} {test_cost_ben_thresh - test_cost_ben_default:+.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save test metrics to CSV for both thresholds\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # Default threshold metrics DataFrame\n",
    "    test_metrics_default = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model\": best_model_name,\n",
    "                \"threshold\": 0.5,\n",
    "                \"mcc\": test_mcc,\n",
    "                \"accuracy\": test_acc,\n",
    "                \"precision\": test_prec,\n",
    "                \"recall\": test_rec,\n",
    "                \"f1\": test_f1,\n",
    "                \"balanced_accuracy\": test_bal_acc,\n",
    "                \"specificity\": test_spec,\n",
    "                \"cost_benefit\": test_cost_ben_default,\n",
    "                \"cost_benefit_raw\": test_cost_ben_default_raw,\n",
    "                \"roc_auc\": test_roc_auc,\n",
    "                \"pr_auc\": test_pr_auc,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    test_metrics_default.to_csv(os.path.join(tables_folder, \"test_metrics_default_threshold.csv\"), index=False)\n",
    "    print(f\"Saved default threshold metrics to: {os.path.join(tables_folder, 'test_metrics_default_threshold.csv')}\")\n",
    "\n",
    "    # Optimal threshold metrics DataFrame\n",
    "    test_metrics_optimal = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model\": best_model_name,\n",
    "                \"threshold\": optimal_threshold_to_use,\n",
    "                \"mcc\": test_mcc_thresh,\n",
    "                \"accuracy\": test_acc_thresh,\n",
    "                \"precision\": test_prec_thresh,\n",
    "                \"recall\": test_rec_thresh,\n",
    "                \"f1\": test_f1_thresh,\n",
    "                \"balanced_accuracy\": test_bal_acc_thresh,\n",
    "                \"specificity\": test_spec_thresh,\n",
    "                \"cost_benefit\": test_cost_ben_thresh,\n",
    "                \"cost_benefit_raw\": test_cost_ben_thresh_raw,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    test_metrics_optimal.to_csv(os.path.join(tables_folder, \"test_metrics_optimal_threshold.csv\"), index=False)\n",
    "    print(f\"Saved optimal threshold metrics to: {os.path.join(tables_folder, 'test_metrics_optimal_threshold.csv')}\")\n",
    "\n",
    "    # Classification reports\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CLASSIFICATION REPORT (Default Threshold 0.5)\")\n",
    "    print(\"=\" * 70)\n",
    "    report_default = classification_report(y_test, y_test_pred, target_names=lb.classes_, output_dict=True)\n",
    "    print(classification_report(y_test, y_test_pred, target_names=lb.classes_))\n",
    "\n",
    "    # Save as CSV\n",
    "    report_default_df = pd.DataFrame(report_default).transpose()\n",
    "    report_default_df.to_csv(os.path.join(tables_folder, \"classification_report_default_threshold.csv\"))\n",
    "    print(f\"Saved classification report to: {os.path.join(tables_folder, 'classification_report_default_threshold.csv')}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"CLASSIFICATION REPORT (Optimal Threshold {optimal_threshold_to_use:.2f})\")\n",
    "    print(\"=\" * 70)\n",
    "    report_optimal = classification_report(y_test, y_test_pred_thresholded, target_names=lb.classes_, output_dict=True)\n",
    "    print(classification_report(y_test, y_test_pred_thresholded, target_names=lb.classes_))\n",
    "\n",
    "    # Save as CSV\n",
    "    report_optimal_df = pd.DataFrame(report_optimal).transpose()\n",
    "    report_optimal_df.to_csv(os.path.join(tables_folder, \"classification_report_optimal_threshold.csv\"))\n",
    "    print(f\"Saved classification report to: {os.path.join(tables_folder, 'classification_report_optimal_threshold.csv')}\")\n",
    "\n",
    "    # Confusion matrix visualization with optimal threshold\n",
    "    cm_thresh = confusion_matrix(y_test, y_test_pred_thresholded)\n",
    "    disp_thresh = ConfusionMatrixDisplay(confusion_matrix=cm_thresh, display_labels=lb.classes_)\n",
    "    disp_thresh.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    ax = plt.gca()\n",
    "    ax.grid(False)\n",
    "    plt.title(f\"Confusion matrix (Optimal threshold={optimal_threshold_to_use:.2f})\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_folder, f\"confusion_matrix_optimal_threshold_{out_suffix}.pdf\")\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Saved confusion matrix figure to: {out_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Update the best_pipeline to use the optimal threshold\n",
    "    best_pipeline_final = best_pipeline_thresholded\n",
    "    print(f\"\\nFinal model uses optimal threshold: {optimal_threshold_to_use:.3f}\")\n",
    "else:\n",
    "    best_pipeline_final = best_pipeline\n",
    "    print(\"Using default threshold (0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b40a90",
   "metadata": {},
   "source": [
    "## 17. Ensemble Methods: Combining Multiple Models\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand ensemble learning principles\n",
    "- Compare voting vs stacking strategies\n",
    "- Learn when ensembles outperform single models\n",
    "- Apply threshold optimization to ensemble predictions\n",
    "\n",
    "**Ensemble learning** combines predictions from multiple models to improve performance. Two main approaches:\n",
    "\n",
    "1. **Voting Classifier**: Combines predictions by majority vote (hard) or averaging probabilities (soft)\n",
    "2. **Stacking Classifier**: Trains a meta-model to optimally combine base model predictions\n",
    "\n",
    "**When do ensembles help?**\n",
    "- Diverse base models (different algorithms, hyperparameters, or training data)\n",
    "- Base models perform reasonably well but make different errors\n",
    "- Sufficient computational resources for training multiple models\n",
    "\n",
    "**Common Pitfall:** Ensembling many poor models rarely helps. Focus on combining strong, diverse models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENSEMBLE_METHOD and TOP_N_MODELS_FOR_ENSEMBLE > 1:\n",
    "    print(f\"Creating ensemble from top {TOP_N_MODELS_FOR_ENSEMBLE} models using {ENSEMBLE_METHOD} strategy\")\n",
    "\n",
    "    # Select top N models based on validation MCC\n",
    "    top_models = model_comparison_df.nlargest(TOP_N_MODELS_FOR_ENSEMBLE, \"mean_mcc\")\n",
    "    print(f\"\\nTop {TOP_N_MODELS_FOR_ENSEMBLE} models selected:\")\n",
    "    print(top_models[[\"model\", \"mean_mcc\", \"std_mcc\"]])\n",
    "\n",
    "    # Recreate pipelines for top models\n",
    "    ensemble_estimators = []\n",
    "    for _idx, row in top_models.iterrows():\n",
    "        model_name = row[\"model\"]\n",
    "        # Get original model configuration (copy to avoid mutating stored params)\n",
    "        params_dict = best_params_by_model.get(model_name, {}).copy()\n",
    "        sampling_strategy = params_dict.pop(\"sampling_strategy\", \"none\")\n",
    "\n",
    "        # Convert hidden_layer_sizes from string to tuple if needed (for MLPClassifier)\n",
    "        if (\n",
    "            \"hidden_layer_sizes\" in params_dict\n",
    "            and isinstance(params_dict[\"hidden_layer_sizes\"], str)\n",
    "            and \"-\" in params_dict[\"hidden_layer_sizes\"]\n",
    "        ):\n",
    "            params_dict[\"hidden_layer_sizes\"] = tuple(map(int, params_dict[\"hidden_layer_sizes\"].split(\"-\")))\n",
    "\n",
    "        # Create base estimator\n",
    "        base_est = create_base_estimator(model_name, params=params_dict)\n",
    "        # base_est.set_params(**params_dict)\n",
    "\n",
    "        # Build pipeline\n",
    "        pipe = build_pipeline(base_est, sampling_strategy=sampling_strategy)\n",
    "\n",
    "        # Fit on full training set\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        # Add to ensemble list (use short name for clarity)\n",
    "        ensemble_estimators.append((model_name[:10], pipe))\n",
    "        print(f\"  - Fitted {model_name} for ensemble\")\n",
    "\n",
    "    # Create ensemble based on configuration\n",
    "    if ENSEMBLE_METHOD == \"voting\":\n",
    "        ensemble_model = VotingClassifier(\n",
    "            estimators=ensemble_estimators,\n",
    "            voting=\"soft\",  # Use probability averaging\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        print(f\"\\nCreated VotingClassifier with soft voting\")\n",
    "\n",
    "    elif ENSEMBLE_METHOD == \"stacking\":\n",
    "        # Use logistic regression as meta-classifier\n",
    "        # meta_classifier = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "        # Use GradientBoostingClassifier as meta-classifier\n",
    "        # print(f\"\\nCreated StackingClassifier with LogisticRegression meta-classifier\")\n",
    "        meta_classifier = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "        ensemble_model = StackingClassifier(estimators=ensemble_estimators, final_estimator=meta_classifier, cv=5, n_jobs=-1)\n",
    "        print(f\"\\nCreated StackingClassifier with GradientBoostingClassifier meta-classifier\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown ENSEMBLE_METHOD: {ENSEMBLE_METHOD}. Skipping ensemble.\")\n",
    "        ensemble_model = None\n",
    "\n",
    "    if ensemble_model is not None:\n",
    "        # Fit ensemble on training data\n",
    "        print(\"\\nFitting ensemble model...\")\n",
    "        ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test set\n",
    "        # Suppress feature names warning - this occurs because the meta-classifier is fitted\n",
    "        # on stacked predictions (numpy array without feature names) but receives predictions\n",
    "        # from base estimators that were fitted on DataFrames with feature names\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"X has feature names, but .* was fitted without feature names\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            y_test_pred_ensemble = ensemble_model.predict(X_test)\n",
    "            y_test_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Evaluate ensemble with default threshold\n",
    "        ensemble_mcc = matthews_corrcoef(y_test, y_test_pred_ensemble)\n",
    "        ensemble_acc = accuracy_score(y_test, y_test_pred_ensemble)\n",
    "        ensemble_prec = precision_score(y_test, y_test_pred_ensemble, zero_division=0)\n",
    "        ensemble_rec = recall_score(y_test, y_test_pred_ensemble, zero_division=0)\n",
    "        ensemble_f1 = f1_score(y_test, y_test_pred_ensemble, zero_division=0)\n",
    "        ensemble_bal_acc = balanced_accuracy_score(y_test, y_test_pred_ensemble)\n",
    "        ensemble_spec = specificity_score(y_test, y_test_pred_ensemble)\n",
    "        ensemble_cost_ben_raw = cost_benefit_score(y_test, y_test_pred_ensemble, COST_MATRIX, normalize=False)\n",
    "        ensemble_cost_ben = cost_benefit_score(y_test, y_test_pred_ensemble, COST_MATRIX, normalize=True)\n",
    "\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"ENSEMBLE PERFORMANCE (default threshold = 0.5)\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"MCC:              {ensemble_mcc:.4f}\")\n",
    "        print(f\"Accuracy:         {ensemble_acc:.4f}\")\n",
    "        print(f\"Precision:        {ensemble_prec:.4f}\")\n",
    "        print(f\"Recall:           {ensemble_rec:.4f}\")\n",
    "        print(f\"F1:               {ensemble_f1:.4f}\")\n",
    "        print(f\"Balanced Acc:     {ensemble_bal_acc:.4f}\")\n",
    "        print(f\"Specificity:      {ensemble_spec:.4f}\")\n",
    "        print(f\"Cost-Benefit:     {ensemble_cost_ben:.4f}\")\n",
    "\n",
    "        # Confusion matrix for ensemble (default threshold)\n",
    "        cm_ensemble = confusion_matrix(y_test, y_test_pred_ensemble)\n",
    "        disp_ensemble = ConfusionMatrixDisplay(confusion_matrix=cm_ensemble, display_labels=lb.classes_)\n",
    "        disp_ensemble.plot(cmap=\"Greens\", values_format=\"d\")\n",
    "        ax = plt.gca()\n",
    "        ax.grid(False)\n",
    "        plt.title(f\"Confusion matrix: Ensemble ({ENSEMBLE_METHOD}) - Default threshold=0.5\")\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(out_folder, f\"confusion_matrix_ensemble_default_threshold_{out_suffix}.pdf\")\n",
    "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved ensemble confusion matrix to: {out_path}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Apply threshold optimization to ensemble if enabled\n",
    "        if USE_OPTIMAL_THRESHOLD:\n",
    "            print(f\"\\n{'=' * 70}\")\n",
    "            print(f\"APPLYING OPTIMAL THRESHOLD TO ENSEMBLE\")\n",
    "            print(f\"{'=' * 70}\")\n",
    "\n",
    "            # Find optimal threshold for ensemble predictions\n",
    "            ensemble_thresholds = np.linspace(0.0, 1.0, 101)\n",
    "            ensemble_th_records = []\n",
    "\n",
    "            for thr in ensemble_thresholds:\n",
    "                y_thr = (y_test_proba_ensemble >= thr).astype(int)\n",
    "                mcc_thr = matthews_corrcoef(y_test, y_thr)\n",
    "                f1_thr = f1_score(y_test, y_thr, zero_division=0)\n",
    "                bal_acc_thr = balanced_accuracy_score(y_test, y_thr)\n",
    "                cost_ben_raw = cost_benefit_score(y_test, y_thr, COST_MATRIX, normalize=False)\n",
    "                cost_ben_norm = cost_benefit_score(y_test, y_thr, COST_MATRIX, normalize=True)\n",
    "                ensemble_th_records.append(\n",
    "                    {\n",
    "                        \"threshold\": thr,\n",
    "                        \"mcc\": mcc_thr,\n",
    "                        \"f1\": f1_thr,\n",
    "                        \"balanced_accuracy\": bal_acc_thr,\n",
    "                        \"cost_benefit_raw\": cost_ben_raw,\n",
    "                        \"cost_benefit\": cost_ben_norm,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            ensemble_threshold_df = pd.DataFrame(ensemble_th_records)\n",
    "\n",
    "            # Save ensemble threshold analysis to CSV\n",
    "            ensemble_threshold_csv_path = os.path.join(tables_folder, \"ensemble_threshold_analysis.csv\")\n",
    "            ensemble_threshold_df.to_csv(ensemble_threshold_csv_path, index=False)\n",
    "            print(f\"Saved ensemble threshold analysis to: {ensemble_threshold_csv_path}\")\n",
    "\n",
    "            # Find optimal threshold based on configuration\n",
    "            ensemble_optimal_thresholds = {}\n",
    "            for metric in [\"mcc\", \"f1\", \"balanced_accuracy\", \"cost_benefit\"]:\n",
    "                optimal_idx = ensemble_threshold_df[metric].idxmax()\n",
    "                ensemble_optimal_thresholds[metric] = ensemble_threshold_df.loc[optimal_idx, \"threshold\"]\n",
    "\n",
    "            # Use same metric as before\n",
    "            ensemble_optimal_threshold = ensemble_optimal_thresholds.get(threshold_metric_to_use, 0.5)\n",
    "            print(f\"Optimal ensemble threshold for {threshold_metric_to_use}: {ensemble_optimal_threshold:.3f}\")\n",
    "\n",
    "            # Create thresholded ensemble classifier\n",
    "            ensemble_model_thresholded = ThresholdedClassifier(ensemble_model, threshold=ensemble_optimal_threshold)\n",
    "\n",
    "            # Re-evaluate with optimal threshold\n",
    "            y_test_pred_ensemble_thresh = ensemble_model_thresholded.predict(X_test)\n",
    "\n",
    "            ensemble_mcc_thresh = matthews_corrcoef(y_test, y_test_pred_ensemble_thresh)\n",
    "            ensemble_acc_thresh = accuracy_score(y_test, y_test_pred_ensemble_thresh)\n",
    "            ensemble_prec_thresh = precision_score(y_test, y_test_pred_ensemble_thresh, zero_division=0)\n",
    "            ensemble_rec_thresh = recall_score(y_test, y_test_pred_ensemble_thresh, zero_division=0)\n",
    "            ensemble_f1_thresh = f1_score(y_test, y_test_pred_ensemble_thresh, zero_division=0)\n",
    "            ensemble_bal_acc_thresh = balanced_accuracy_score(y_test, y_test_pred_ensemble_thresh)\n",
    "            ensemble_spec_thresh = specificity_score(y_test, y_test_pred_ensemble_thresh)\n",
    "            ensemble_cost_ben_thresh_raw = cost_benefit_score(\n",
    "                y_test, y_test_pred_ensemble_thresh, COST_MATRIX, normalize=False\n",
    "            )\n",
    "            ensemble_cost_ben_thresh = cost_benefit_score(y_test, y_test_pred_ensemble_thresh, COST_MATRIX, normalize=True)\n",
    "\n",
    "            print(\n",
    "                f\"\\n{'Metric':<20} {'Default (0.5)':<15} {'Optimal (' + f'{ensemble_optimal_threshold:.2f}' + ')':<15} {'Improvement':<15}\"\n",
    "            )\n",
    "            print(\"=\" * 70)\n",
    "            print(\n",
    "                f\"{'MCC':<20} {ensemble_mcc:<15.4f} {ensemble_mcc_thresh:<15.4f} {ensemble_mcc_thresh - ensemble_mcc:+.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{'Accuracy':<20} {ensemble_acc:<15.4f} {ensemble_acc_thresh:<15.4f} {ensemble_acc_thresh - ensemble_acc:+.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{'Precision':<20} {ensemble_prec:<15.4f} {ensemble_prec_thresh:<15.4f} {ensemble_prec_thresh - ensemble_prec:+.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{'Recall':<20} {ensemble_rec:<15.4f} {ensemble_rec_thresh:<15.4f} {ensemble_rec_thresh - ensemble_rec:+.4f}\"\n",
    "            )\n",
    "            print(f\"{'F1':<20} {ensemble_f1:<15.4f} {ensemble_f1_thresh:<15.4f} {ensemble_f1_thresh - ensemble_f1:+.4f}\")\n",
    "            print(\n",
    "                f\"{'Balanced Acc':<20} {ensemble_bal_acc:<15.4f} {ensemble_bal_acc_thresh:<15.4f} {ensemble_bal_acc_thresh - ensemble_bal_acc:+.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{'Specificity':<20} {ensemble_spec:<15.4f} {ensemble_spec_thresh:<15.4f} {ensemble_spec_thresh - ensemble_spec:+.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{'Cost-Benefit':<20} {ensemble_cost_ben:<15.4f} {ensemble_cost_ben_thresh:<15.4f} {ensemble_cost_ben_thresh - ensemble_cost_ben:+.4f}\"\n",
    "            )\n",
    "\n",
    "            # Confusion matrix for ensemble (optimal threshold)\n",
    "            cm_ensemble_thresh = confusion_matrix(y_test, y_test_pred_ensemble_thresh)\n",
    "            disp_ensemble_thresh = ConfusionMatrixDisplay(confusion_matrix=cm_ensemble_thresh, display_labels=lb.classes_)\n",
    "            disp_ensemble_thresh.plot(cmap=\"Purples\", values_format=\"d\")\n",
    "            ax = plt.gca()\n",
    "            ax.grid(False)\n",
    "            plt.title(f\"Confusion matrix: Ensemble ({ENSEMBLE_METHOD}) - Optimal threshold={ensemble_optimal_threshold:.2f}\")\n",
    "            plt.tight_layout()\n",
    "            out_path = os.path.join(out_folder, f\"confusion_matrix_ensemble_optimal_threshold_{out_suffix}.pdf\")\n",
    "            plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "            print(f\"Saved ensemble confusion matrix (optimal threshold) to: {out_path}\")\n",
    "            plt.show()\n",
    "\n",
    "            ensemble_model_final = ensemble_model_thresholded\n",
    "        else:\n",
    "            ensemble_model_final = ensemble_model\n",
    "\n",
    "        # Compare ensemble to best single model\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"ENSEMBLE vs BEST SINGLE MODEL COMPARISON\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        if USE_OPTIMAL_THRESHOLD:\n",
    "            best_model_mcc_final = test_mcc_thresh\n",
    "            ensemble_mcc_final = ensemble_mcc_thresh\n",
    "        else:\n",
    "            best_model_mcc_final = test_mcc\n",
    "            ensemble_mcc_final = ensemble_mcc\n",
    "\n",
    "        print(f\"Best Single Model ({best_name}): MCC = {best_model_mcc_final:.4f}\")\n",
    "        print(f\"Ensemble ({ENSEMBLE_METHOD}):       MCC = {ensemble_mcc_final:.4f}\")\n",
    "\n",
    "        if ensemble_mcc_final > best_model_mcc_final:\n",
    "            improvement = ((ensemble_mcc_final - best_model_mcc_final) / abs(best_model_mcc_final)) * 100\n",
    "            print(f\"\\nEnsemble OUTPERFORMS best single model by {improvement:.2f}%\")\n",
    "            final_model_to_use = ensemble_model_final\n",
    "            final_model_name = f\"Ensemble ({ENSEMBLE_METHOD})\"\n",
    "        else:\n",
    "            decline = ((best_model_mcc_final - ensemble_mcc_final) / abs(best_model_mcc_final)) * 100\n",
    "            print(f\"\\nEnsemble UNDERPERFORMS best single model by {decline:.2f}%\")\n",
    "            print(f\"  Keeping best single model ({best_name})\")\n",
    "            final_model_to_use = best_pipeline_final\n",
    "            final_model_name = best_name\n",
    "\n",
    "else:\n",
    "    print(f\"Ensemble disabled (ENSEMBLE_METHOD={ENSEMBLE_METHOD}, TOP_N={TOP_N_MODELS_FOR_ENSEMBLE})\")\n",
    "    final_model_to_use = best_pipeline_final\n",
    "    final_model_name = best_name\n",
    "\n",
    "print(f\"\\nFinal model selected: {final_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2401726",
   "metadata": {},
   "source": [
    "## 18. Probability Calibration Analysis\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand what probability calibration means\n",
    "- Learn to assess calibration using calibration curves\n",
    "- Compute log loss and Brier score as probabilistic metrics\n",
    "- Recognize when calibration matters vs decision metrics\n",
    "- Understand calibration methods and when to apply them\n",
    "\n",
    "**[Probability calibration](https://scikit-learn.org/stable/modules/calibration.html#calibration)** refers to how well predicted probabilities reflect true frequencies. A perfectly calibrated model with predicted probability 0.7 should be correct 70% of the time.\n",
    "\n",
    "**Why calibration matters:**\n",
    "- Medical diagnosis (need reliable confidence scores)\n",
    "- Financial risk assessment\n",
    "- When probabilities are used directly (not just binary decisions)\n",
    "- Combining predictions from multiple models\n",
    "\n",
    "**Common Pitfall:** Many sklearn models (especially tree-based) produce poorly calibrated probabilities. Decision metrics (MCC, F1) can still be excellent even with poor calibration.\n",
    "\n",
    "Uses [`brier_score_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html) and [`log_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) to assess how well calibrated the prediction p-values are.\n",
    "\n",
    "### Calibration Methods\n",
    "\n",
    "If your model produces poorly calibrated probabilities, you can apply post-hoc calibration using [`CalibratedClassifierCV`](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html):\n",
    "\n",
    "**1. Platt Scaling (Sigmoid Calibration)**\n",
    "- Fits a logistic regression to map raw predictions to calibrated probabilities\n",
    "- Assumes the calibration function is sigmoid-shaped\n",
    "- Works well for: SVM, neural networks, models with sigmoid-like miscalibration\n",
    "- Fast, requires fewer samples\n",
    "\n",
    "**2. Isotonic Regression**\n",
    "- Non-parametric approach that fits a monotonic (non-decreasing) function\n",
    "- Makes no assumptions about the shape of the calibration function\n",
    "- More flexible than Platt scaling\n",
    "- Works well for: Tree-based models, models with complex miscalibration patterns\n",
    "- Requires more data, can overfit with small datasets\n",
    "\n",
    "**When to calibrate:**\n",
    "- When you need reliable probability estimates (not just rankings)\n",
    "- When probabilities will be used for decision-making with costs\n",
    "- When combining models in ensembles\n",
    "- When calibration curve shows significant deviation from diagonal\n",
    "\n",
    "**When calibration is unnecessary:**\n",
    "- When only the ranking/ordering of predictions matters (e.g., for threshold selection)\n",
    "- When using models that are naturally well-calibrated (e.g., Logistic Regression)\n",
    "- When your evaluation metrics are decision-based (MCC, F1) rather than probabilistic\n",
    "\n",
    "For implementation examples, see the [sklearn calibration documentation](https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_multiclass.html).\n",
    "\n",
    "In this section, we assess calibration quality. Applying calibration is left as an extension exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for final model\n",
    "if hasattr(final_model_to_use, \"predict_proba\"):\n",
    "    y_test_proba_final = final_model_to_use.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(final_model_to_use, \"decision_function\"):\n",
    "    # For models without predict_proba, convert decision function to pseudo-probabilities\n",
    "    from scipy.special import expit\n",
    "\n",
    "    y_test_proba_final = expit(final_model_to_use.decision_function(X_test))\n",
    "else:\n",
    "    y_test_proba_final = None\n",
    "    print(\"Final model does not support probability prediction\")\n",
    "\n",
    "if y_test_proba_final is not None:\n",
    "    # Compute probabilistic metrics\n",
    "    test_log_loss = log_loss(y_test, y_test_proba_final)\n",
    "    test_brier_score = brier_score_loss(y_test, y_test_proba_final)\n",
    "\n",
    "    print(f\"Probabilistic Metrics for {final_model_name}:\")\n",
    "    print(f\"  Log Loss:     {test_log_loss:.4f} (lower is better)\")\n",
    "    print(f\"  Brier Score:  {test_brier_score:.4f} (lower is better)\")\n",
    "\n",
    "    # Create calibration curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot perfectly calibrated line\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\", linewidth=2)\n",
    "\n",
    "    # Plot calibration curve for final model\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_test_proba_final, n_bins=10, strategy=\"uniform\")\n",
    "    ax.plot(prob_pred, prob_true, marker=\"o\", linewidth=2, label=final_model_name)\n",
    "\n",
    "    ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "    ax.set_ylabel(\"Fraction of Positives (True Probability)\")\n",
    "    ax.set_title(f\"Calibration Curve: {final_model_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_folder, f\"calibration_curve_{out_suffix}.pdf\")\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"\\nSaved calibration curve to: {out_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Interpretation\n",
    "    print(\"\\nCalibration Interpretation:\")\n",
    "    print(\"- Points close to diagonal = well calibrated\")\n",
    "    print(\"- Points below diagonal = model is overconfident (predicted probabilities too high)\")\n",
    "    print(\"- Points above diagonal = model is underconfident (predicted probabilities too low)\")\n",
    "else:\n",
    "    print(\"Skipping calibration analysis (model does not support probabilities)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255448f",
   "metadata": {},
   "source": [
    "## 19. Feature Importance Analysis\n",
    "\n",
    "### Learning Objectives:\n",
    "- Extract and interpret feature importance\n",
    "- Understand model-specific vs model-agnostic methods\n",
    "- Learn which features drive predictions\n",
    "- Recognize importance limitations\n",
    "\n",
    "**Feature importance** reveals which features most influence model predictions. Different methods:\n",
    "\n",
    "1. **Model-specific importance**: Built-in to tree-based models (Gini importance)\n",
    "2. **Permutation importance**: Model-agnostic, measures performance drop when feature shuffled\n",
    "3. **SHAP values**: Game-theoretic approach providing detailed attributions\n",
    "\n",
    "**Common Pitfall:** Gini importance can be misleading with correlated features and doesn't account for interactions. Use multiple methods for robust conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance if available (tree-based models)\n",
    "\n",
    "# First, unwrap the model if it's a ThresholdedClassifier\n",
    "model_for_importance = final_model_to_use\n",
    "if isinstance(model_for_importance, ThresholdedClassifier):\n",
    "    model_for_importance = model_for_importance.estimator\n",
    "    print(f\"Unwrapped ThresholdedClassifier to access underlying model\")\n",
    "\n",
    "# Handle ensemble models (VotingClassifier, StackingClassifier)\n",
    "if hasattr(model_for_importance, \"estimators_\"):\n",
    "    # For VotingClassifier, get the first base estimator's pipeline\n",
    "    print(\"Ensemble model detected - using first base estimator for feature importance\")\n",
    "    first_estimator = model_for_importance.estimators_[0]\n",
    "    # The estimator might be a (name, pipeline) tuple or just a pipeline\n",
    "    if isinstance(first_estimator, tuple):\n",
    "        first_estimator = first_estimator[1]\n",
    "    model_for_importance = first_estimator\n",
    "\n",
    "# Now check if we have a pipeline with named_steps\n",
    "if hasattr(model_for_importance, \"named_steps\"):\n",
    "    # Get the final estimator from pipeline (step name is \"clf\")\n",
    "    estimator_step = model_for_importance.named_steps.get(\"clf\")\n",
    "    base_estimator = estimator_step\n",
    "\n",
    "    if base_estimator is not None and hasattr(base_estimator, \"feature_importances_\"):\n",
    "        importances = base_estimator.feature_importances_\n",
    "\n",
    "        # Get feature names after preprocessing\n",
    "        preprocessor = model_for_importance.named_steps.get(\"preprocess\")\n",
    "        if preprocessor is None:\n",
    "            preprocessor = model_for_importance.named_steps.get(\"preprocessor\")\n",
    "\n",
    "        if preprocessor is not None:\n",
    "            # Extract feature names from ColumnTransformer\n",
    "            feature_names = []\n",
    "            for name, transformer, columns in preprocessor.transformers_:\n",
    "                if name == \"num\":\n",
    "                    feature_names.extend(columns)\n",
    "                elif name == \"cat\":\n",
    "                    if hasattr(transformer.named_steps[\"onehot\"], \"get_feature_names_out\"):\n",
    "                        cat_features = transformer.named_steps[\"onehot\"].get_feature_names_out(columns)\n",
    "                        feature_names.extend(cat_features)\n",
    "                    else:\n",
    "                        feature_names.extend(columns)\n",
    "\n",
    "            # Create importance dataframe\n",
    "            importance_df = pd.DataFrame(\n",
    "                {\"feature\": feature_names[: len(importances)], \"importance\": importances}\n",
    "            ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "            print(f\"\\nTop 20 Most Important Features ({final_model_name}):\")\n",
    "            print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "            # Plot top 20 features\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_n = min(20, len(importance_df))\n",
    "            plt.barh(range(top_n), importance_df[\"importance\"].iloc[:top_n])\n",
    "            plt.yticks(range(top_n), importance_df[\"feature\"].iloc[:top_n])\n",
    "            plt.xlabel(\"Feature Importance\")\n",
    "            plt.title(f\"Top {top_n} Feature Importances: {final_model_name}\")\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            out_path = os.path.join(out_folder, f\"feature_importance_{out_suffix}.pdf\")\n",
    "            plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "            print(f\"\\nSaved feature importance plot to: {out_path}\")\n",
    "            plt.show()\n",
    "\n",
    "            # Save to CSV\n",
    "            csv_path = os.path.join(tables_folder, f\"feature_importance_{out_suffix}.csv\")\n",
    "            importance_df.to_csv(csv_path, index=False)\n",
    "            print(f\"Saved feature importance data to: {csv_path}\")\n",
    "        else:\n",
    "            print(\"Could not find preprocessor in pipeline - skipping feature importance\")\n",
    "    else:\n",
    "        print(f\"\\n{final_model_name} does not provide feature_importances_\")\n",
    "        print(\"Consider using permutation importance for model-agnostic feature ranking\")\n",
    "else:\n",
    "    print(f\"Final model ({type(model_for_importance).__name__}) is not a pipeline - skipping feature importance\")\n",
    "    print(\"Note: This can happen with some ensemble methods or custom wrappers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681fe89",
   "metadata": {},
   "source": [
    "## 20. Learning Curves: Diagnosing Underfitting and Overfitting\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand learning curves and their interpretation\n",
    "- Diagnose underfitting (high bias)\n",
    "- Diagnose overfitting (high variance)\n",
    "- Learn how training set size affects performance\n",
    "\n",
    "**[Learning curves](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)** plot training and validation performance vs training set size. They reveal:\n",
    "\n",
    "- **Underfitting**: Both curves plateau at low performance (need more model capacity)\n",
    "- **Overfitting**: Large gap between training and validation (need more data or regularization)\n",
    "- **Well-fit**: Both curves converge at high performance\n",
    "\n",
    "**Common Pitfall:** Computing learning curves on the test set causes overfitting. Always use cross-validation on training data only.\n",
    "\n",
    "See https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html for an explanation of these plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computing learning curves for {best_name} (this may take time in non-QUICK_MODE)...\")\n",
    "\n",
    "# Use the best single model (not ensemble) for clearer interpretation\n",
    "# Get model without threshold wrapper if applied\n",
    "if isinstance(best_pipeline_final, ThresholdedClassifier):\n",
    "    model_for_learning_curve = best_pipeline_final.estimator\n",
    "else:\n",
    "    model_for_learning_curve = best_pipeline_final\n",
    "\n",
    "# Define training sizes\n",
    "if QUICK_MODE:\n",
    "    train_sizes = np.linspace(0.3, 1.0, 5)\n",
    "    cv_for_learning = 3\n",
    "else:\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    cv_for_learning = 5\n",
    "\n",
    "# Compute learning curves using cross-validation\n",
    "train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "    model_for_learning_curve,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=cv_for_learning,\n",
    "    scoring=\"matthews_corrcoef\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Compute mean and std\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "val_scores_mean = val_scores.mean(axis=1)\n",
    "val_scores_std = val_scores.std(axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(\n",
    "    train_sizes_abs, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"blue\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    train_sizes_abs, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.1, color=\"orange\"\n",
    ")\n",
    "plt.plot(train_sizes_abs, train_scores_mean, \"o-\", color=\"blue\", label=\"Training score\", linewidth=2)\n",
    "plt.plot(train_sizes_abs, val_scores_mean, \"o-\", color=\"orange\", label=\"Validation score\", linewidth=2)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"MCC Score\")\n",
    "plt.title(f\"Learning Curves: {best_name}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = os.path.join(out_folder, f\"learning_curves_{out_suffix}.pdf\")\n",
    "plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nSaved learning curves to: {out_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "final_gap = train_scores_mean[-1] - val_scores_mean[-1]\n",
    "print(\"\\nLearning Curve Interpretation:\")\n",
    "print(f\"  Final training score:   {train_scores_mean[-1]:.4f}\")\n",
    "print(f\"  Final validation score: {val_scores_mean[-1]:.4f}\")\n",
    "print(f\"  Gap:                    {final_gap:.4f}\")\n",
    "\n",
    "if final_gap > 0.1:\n",
    "    print(\"  → Model shows signs of OVERFITTING (large gap)\")\n",
    "    print(\"    Consider: more data, stronger regularization, or simpler model\")\n",
    "elif val_scores_mean[-1] < 0.5:\n",
    "    print(\"  → Model shows signs of UNDERFITTING (low validation score)\")\n",
    "    print(\"    Consider: more complex model, more features, or less regularization\")\n",
    "else:\n",
    "    print(\"  → Model appears reasonably well-fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3900f",
   "metadata": {},
   "source": [
    "## 21. Wrap-up and Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this comprehensive Coding Exercise 2 notebook, we built a production-quality ML pipeline covering:\n",
    "\n",
    "**1. Data Preparation and Leakage Prevention**\n",
    "- Built leakage-resistant workflows using [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "- Proper train/validation/test splits with stratification\n",
    "\n",
    "**2. Handling Class Imbalance**\n",
    "- Integrated **imbalanced-learn** with 8 sampling strategies (SMOTE, ADASYN, undersampling, etc.)\n",
    "- Made sampling strategy a hyperparameter for Optuna to optimize\n",
    "- Used imbalanced-aware metrics (MCC, balanced accuracy, F1)\n",
    "\n",
    "**3. Hyperparameter Tuning with Optuna**\n",
    "- Tuned 6+ models using Bayesian optimization (TPE sampler)\n",
    "- Configured objective function (MCC, F1, balanced_accuracy, or cost_benefit)\n",
    "- Analyzed parameter importance using fANOVA\n",
    "- Visualized optimization with parallel coordinates plots\n",
    "\n",
    "**4. Comprehensive Model Evaluation**\n",
    "- Evaluated 10+ metrics: MCC, accuracy, precision, recall, F1, balanced accuracy, specificity, ROC AUC, PR AUC, cost-benefit\n",
    "- Cross-validation for robust performance estimates\n",
    "- Confusion matrices and ROC/PR curves\n",
    "\n",
    "**5. Threshold Optimization**\n",
    "- Moved beyond default 0.5 threshold\n",
    "- Found optimal thresholds for different objectives\n",
    "- Implemented `ThresholdedClassifier` wrapper\n",
    "- Visualized precision-recall trade-offs\n",
    "\n",
    "**6. Ensemble Learning**\n",
    "- Combined top N models using voting or stacking\n",
    "- Compared ensemble vs best single model\n",
    "- Applied threshold optimization to ensembles\n",
    "\n",
    "**7. Model Interpretability**\n",
    "- Assessed probability calibration (calibration curves, log loss, Brier score)\n",
    "- Analyzed feature importance for tree-based models\n",
    "- Generated learning curves to diagnose overfitting/underfitting\n",
    "\n",
    "**8. Cost-Benefit Integration**\n",
    "- Implemented cost-benefit analysis with configurable cost matrix\n",
    "- Made cost-benefit available as tuning objective\n",
    "- Supported multiclass extension for generalization\n",
    "\n",
    "### Key Takeaways for Students\n",
    "\n",
    "1. **Metric selection matters**: Different metrics optimize for different goals. MCC is excellent for imbalanced data, but consider misclassification costs when available.\n",
    "\n",
    "2. **Class imbalance requires attention**: Don't ignore class distribution. Use sampling techniques and imbalanced-aware metrics.\n",
    "\n",
    "3. **Threshold tuning is powerful**: The default 0.5 threshold is rarely optimal. Always analyze threshold effects for your specific objective.\n",
    "\n",
    "4. **Ensembles aren't magic**: They work when base models are strong and diverse. Sometimes a single well-tuned model suffices.\n",
    "\n",
    "5. **Interpretability builds trust**: Calibration, feature importance, and learning curves help understand and debug models.\n",
    "\n",
    "6. **Avoid data leakage**: Always use pipelines to ensure preprocessing happens within cross-validation folds.\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "To extend this workflow, consider:\n",
    "- **SHAP values** for detailed feature attributions\n",
    "- **Permutation importance** as model-agnostic alternative\n",
    "- **Partial dependence plots** to understand feature effects\n",
    "- **Bayesian hyperparameter optimization** with more sophisticated priors\n",
    "- **Nested cross-validation** for unbiased hyperparameter selection evaluation\n",
    "- **Calibration methods** (Platt scaling, isotonic regression) to improve probability estimates\n",
    "\n",
    "This notebook extends Coding Exercise 1 from a simple accuracy-focused lifecycle to a comprehensive, production-ready ML workflow suitable for real-world applications.\n",
    "\n",
    "\n",
    "Congratulations on completing this advanced exercise in AI and ML! You've built a robust pipeline that addresses many challenges faced in practical machine learning projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3.14.2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
