{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7423b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Matthew Care\"\n",
    "__version__ = \"0.0.2\"\n",
    "__date__ = \"2026-02-04\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b1405",
   "metadata": {},
   "source": [
    "# Coding Exercise 2 (Advanced) - Pipelines, Metrics and Hyperparameter Tuning\n",
    "\n",
    "## About This Version\n",
    "\n",
    "This is the **Advanced** version of Coding Exercise 2. It covers:\n",
    "- ML pipelines with preprocessing and resampling (SMOTE)\n",
    "- Multiple classification metrics for imbalanced data\n",
    "- Hyperparameter tuning with Optuna\n",
    "- Threshold optimisation\n",
    "\n",
    "**For additional topics** including ensemble methods, probability calibration, feature importance\n",
    "analysis, and learning curves, see `03_ce2_adult_census_income_pipeline_expert.py`.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "1. How to build production-ready ML pipelines that prevent data leakage\n",
    "2. How to handle imbalanced datasets using SMOTE resampling\n",
    "3. How to select and interpret appropriate evaluation metrics for imbalanced classification\n",
    "4. How to tune hyperparameters systematically using Optuna\n",
    "5. How to optimise decision thresholds for different objectives\n",
    "\n",
    "## 0. Setup\n",
    "\n",
    "### Configuration Guide\n",
    "\n",
    "Key parameters you can modify:\n",
    "\n",
    "- `QUICK_MODE`: Set to `True` for faster runs, `False` for full evaluation\n",
    "- `MODEL_NAMES`: List of models to evaluate\n",
    "- `TUNED_MODELS`: Models to hyperparameter tune with Optuna\n",
    "- `TUNING_OBJECTIVE`: Choose from `\"mcc\"`, `\"f1\"`, or `\"balanced_accuracy\"`\n",
    "- `USE_OPTIMAL_THRESHOLD`: Whether to find and apply optimal decision threshold\n",
    "\n",
    "**Reproducibility:** All random operations use `RANDOM_STATE` for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859236e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and global parameters\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.5  # 50% train / 50% test\n",
    "\n",
    "# Quick mode for faster development and testing\n",
    "QUICK_MODE = True  # Set to True for faster runs with reduced trials/splits\n",
    "\n",
    "# Cross-validation configuration\n",
    "N_SPLITS = 3 if QUICK_MODE else 5\n",
    "N_REPEATS_TUNING = 1\n",
    "N_REPEATS_CV = 1 if QUICK_MODE else 2\n",
    "\n",
    "# Parallelism\n",
    "NUM_JOBS = -1  # -1 means use all available cores\n",
    "\n",
    "# Seeds\n",
    "SEED_TUNING = RANDOM_STATE\n",
    "SEED_CV = RANDOM_STATE\n",
    "\n",
    "out_folder = \"coding_exercise_2_advanced\"\n",
    "optuna_folder = \"coding_exercise_2_advanced/optuna\"\n",
    "tables_folder = \"coding_exercise_2_advanced/tables\"\n",
    "out_suffix = \"_adv\"\n",
    "\n",
    "# Optimization objective configuration\n",
    "TUNING_OBJECTIVE = \"mcc\"  # Options: \"mcc\", \"f1\", \"balanced_accuracy\"\n",
    "\n",
    "# Threshold optimization configuration\n",
    "USE_OPTIMAL_THRESHOLD = True\n",
    "\n",
    "# Model configuration\n",
    "ALL_MODELS = [\n",
    "    \"DummyMostFreq\",\n",
    "    \"LogisticRegression\",\n",
    "    \"RandomForest\",\n",
    "    \"LightGBM\",\n",
    "]\n",
    "\n",
    "# Sampling strategies - simplified for advanced version\n",
    "SAMPLING_METHODS = [\"none\", \"smote\"]\n",
    "\n",
    "if QUICK_MODE:\n",
    "    MODEL_NAMES = [\"DummyMostFreq\", \"RandomForest\", \"LightGBM\"]\n",
    "    TUNED_MODELS = [\"LightGBM\"]\n",
    "else:\n",
    "    MODEL_NAMES = ALL_MODELS\n",
    "    TUNED_MODELS = [\"LightGBM\", \"RandomForest\"]\n",
    "\n",
    "# Validation\n",
    "if not set(TUNED_MODELS).issubset(set(MODEL_NAMES)):\n",
    "    missing = set(TUNED_MODELS) - set(MODEL_NAMES)\n",
    "    raise ValueError(f\"TUNED_MODELS contains models not in MODEL_NAMES: {missing}\")\n",
    "\n",
    "N_TRIALS_PER_MODEL = {\n",
    "    \"DummyMostFreq\": 1,\n",
    "    \"LogisticRegression\": 20 if not QUICK_MODE else 5,\n",
    "    \"RandomForest\": 50 if not QUICK_MODE else 10,\n",
    "    \"LightGBM\": 50 if not QUICK_MODE else 15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96080c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    import subprocess\n",
    "\n",
    "    packages = [\"optuna\", \"imbalanced-learn\", \"lightgbm\"]\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder, RobustScaler\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "os.makedirs(optuna_folder, exist_ok=True)\n",
    "os.makedirs(tables_folder, exist_ok=True)\n",
    "\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63468fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Adult Census Income dataset\n",
    "DATA_URL = \"https://github.com/medmaca/shared_data/raw/8a3fea5467ec68b17fd8369c6f77f8016b1ed5f8/Datasets/Kaggle/adult_census_income/adult.csv.zip\"\n",
    "\n",
    "adult_ci_df = pd.read_csv(DATA_URL, compression=\"zip\")\n",
    "adult_ci_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a5524",
   "metadata": {},
   "source": [
    "## 1. Minimal Dataset Checks\n",
    "\n",
    "For full EDA, see Coding Exercise 1. Here we verify the data and check class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_ci_df.info()\n",
    "\n",
    "target_col = \"income\"\n",
    "\n",
    "class_counts = adult_ci_df[target_col].value_counts().sort_index()\n",
    "class_props = adult_ci_df[target_col].value_counts(normalize=True).sort_index()\n",
    "print(f\"\\nClass counts:{class_counts}\")\n",
    "print(f\"\\nClass proportions:{class_props}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6561a7",
   "metadata": {},
   "source": [
    "### Understanding Class Imbalance\n",
    "\n",
    "The dataset shows approximately 75% <=50K and 25% >50K (3:1 imbalance). This means:\n",
    "- A naive classifier predicting \"<=50K\" would achieve 75% accuracy\n",
    "- Accuracy alone is misleading for imbalanced data\n",
    "- We need metrics that account for both classes equally\n",
    "\n",
    "**Common Pitfall:** Using accuracy as the primary metric for imbalanced data leads to\n",
    "models that predict only the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2d86b",
   "metadata": {},
   "source": [
    "## 2. Understanding Classification Metrics\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "All binary classification metrics derive from the confusion matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "& \\text{Predicted Negative} & \\text{Predicted Positive} \\\\\n",
    "\\hline\n",
    "\\text{Actual Negative} & TN & FP \\\\\n",
    "\\text{Actual Positive} & FN & TP \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Formula | Best For |\n",
    "|--------|---------|----------|\n",
    "| **Accuracy** | (TP+TN) / Total | Balanced classes only |\n",
    "| **Precision** | TP / (TP+FP) | When FP is costly |\n",
    "| **Recall** | TP / (TP+FN) | When FN is costly |\n",
    "| **Specificity** | TN/ (TN + FP) | How may negatives did we correctly identify |\n",
    "| **F1 Score** | 2×(Prec×Rec)/(Prec+Rec) | Balance precision/recall |\n",
    "| **MCC** | See formula below | Imbalanced data |\n",
    "| **Balanced Accuracy** | (Recall+Specificity)/2 | Imbalanced data |\n",
    "\n",
    "**[Matthews Correlation Coefficient (MCC)](https://en.wikipedia.org/wiki/Phi_coefficient):**\n",
    "\n",
    "$$\n",
    "\\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "$$\n",
    "\n",
    "[MCC](https://en.wikipedia.org/wiki/Phi_coefficient) ranges from -1 to +1, with 0 being random prediction. It uses all four confusion\n",
    "matrix values and is robust to class imbalance.\n",
    "\n",
    "For cost-benefit analysis and multiclass extensions, see `03_ce2_adult_census_income_pipeline_expert.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76122dc1",
   "metadata": {},
   "source": [
    "## 3. Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68032819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' with NaN\n",
    "adult_ci_df = adult_ci_df.replace(\"?\", np.nan)\n",
    "\n",
    "feature_cols = [c for c in adult_ci_df.columns if c != target_col]\n",
    "categorical_features = [c for c in feature_cols if adult_ci_df[c].dtype == \"object\"]\n",
    "numeric_features = [c for c in feature_cols if adult_ci_df[c].dtype != \"object\"]\n",
    "\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "print(\"Numeric features:\", numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf5622",
   "metadata": {},
   "source": [
    "## 4. Encode Target and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "adult_ci_df[\"target\"] = lb.fit_transform(adult_ci_df[target_col].str.strip()).ravel()\n",
    "print(\"Target classes (lb.classes_):\", lb.classes_)\n",
    "\n",
    "X = adult_ci_df[feature_cols].copy()\n",
    "y = adult_ci_df[\"target\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee8fc7",
   "metadata": {},
   "source": [
    "## 5. Handling Class Imbalance with SMOTE\n",
    "\n",
    "**SMOTE (Synthetic Minority [Over-sampling Technique](https://imbalanced-learn.org/stable/over_sampling.html))** creates synthetic samples by\n",
    "interpolating between minority class neighbours.\n",
    "\n",
    "**Critical Rules:**\n",
    "1. Only resample training data - never test/validation data\n",
    "2. Resample after train/test split to prevent data leakage\n",
    "3. Use `imblearn.pipeline.Pipeline` to ensure resampling happens within CV folds\n",
    "\n",
    "For additional sampling methods (ADASYN, undersampling, hybrid methods), see `03_ce2_adult_census_income_pipeline_expert.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1090e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 6. Utility Functions\n",
    "\n",
    "We define helper functions to compute metrics not available in sklearn or to\n",
    "encapsulate repeated operations. Organising code into functions improves:\n",
    "- **Readability**: Each function has a single, clear purpose\n",
    "- **Testability**: Functions can be tested independently\n",
    "- **Reusability**: The same function can be called from multiple places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    \"\"\"Calculate specificity (true negative rate).\n",
    "\n",
    "    Specificity measures the proportion of actual negatives that were correctly\n",
    "    identified. Also known as the True Negative Rate (TNR).\n",
    "\n",
    "    Args:\n",
    "        y_true: Array-like of true binary labels (0 or 1).\n",
    "        y_pred: Array-like of predicted binary labels (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        float: Specificity score in range [0, 1], or np.nan if computation\n",
    "            is not possible (e.g., no negative samples).\n",
    "\n",
    "    Example:\n",
    "        >>> specificity_score([0, 0, 1, 1], [0, 1, 1, 1])\n",
    "        0.5  # 1 of 2 negatives correctly identified\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape != (2, 2):\n",
    "        return np.nan\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    denom = tn + fp\n",
    "    return tn / denom if denom > 0 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5780e0",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline applies different transformations to numeric and\n",
    "categorical columns. We use sklearn's `ColumnTransformer` to route each\n",
    "column type to the appropriate transformer.\n",
    "\n",
    "**Design Pattern: Factory Function**\n",
    "\n",
    "The `build_pipeline()` function is a *factory* - it creates and returns\n",
    "pipeline objects. This pattern allows us to:\n",
    "- Create pipelines with different configurations (with/without SMOTE)\n",
    "- Ensure consistent preprocessing across all experiments\n",
    "- Easily swap the estimator without duplicating pipeline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e74341",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def build_pipeline(estimator, sampling_strategy=\"none\"):\n",
    "    \"\"\"Build a preprocessing pipeline with optional SMOTE resampling.\n",
    "\n",
    "    Creates a complete ML pipeline that chains preprocessing, optional\n",
    "    resampling, and the classifier. Uses imbalanced-learn's Pipeline when\n",
    "    SMOTE is needed (as sklearn's Pipeline doesn't support resamplers).\n",
    "\n",
    "    Args:\n",
    "        estimator: A scikit-learn compatible classifier instance.\n",
    "        sampling_strategy: Resampling method to apply. Options:\n",
    "            - \"none\": No resampling, uses standard sklearn Pipeline.\n",
    "            - \"smote\": Apply SMOTE oversampling, uses imblearn Pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: A fitted-ready pipeline object (sklearn or imblearn).\n",
    "\n",
    "    Example:\n",
    "        >>> from sklearn.ensemble import RandomForestClassifier\n",
    "        >>> pipe = build_pipeline(RandomForestClassifier(), sampling_strategy=\"smote\")\n",
    "        >>> pipe.fit(X_train, y_train)\n",
    "    \"\"\"\n",
    "    steps = [(\"preprocess\", preprocessor)]\n",
    "\n",
    "    if sampling_strategy == \"smote\":\n",
    "        sampler = SMOTE(random_state=RANDOM_STATE)\n",
    "        steps.append((\"sampler\", sampler))\n",
    "        steps.append((\"clf\", estimator))\n",
    "        return ImbPipeline(steps=steps)\n",
    "    else:\n",
    "        steps.append((\"clf\", estimator))\n",
    "        return Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b04456",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1455c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 8. Model Zoo and Hyperparameter Search Spaces\n",
    "\n",
    "This section defines two key functions that work together:\n",
    "\n",
    "1. **`create_base_estimator()`**: A factory function that creates classifier\n",
    "   instances with sensible default parameters. This centralises model creation\n",
    "   so defaults are consistent throughout the code.\n",
    "\n",
    "2. **`suggest_params_for_model()`**: Defines the hyperparameter search space\n",
    "   for each model. Optuna calls this to get parameter suggestions for each trial.\n",
    "\n",
    "**Why separate these functions?**\n",
    "- `create_base_estimator()` is used both for tuning and final model creation\n",
    "- `suggest_params_for_model()` encapsulates the Optuna trial interface\n",
    "- This separation makes it easy to add new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_estimator(model_name, params=None):\n",
    "    \"\"\"Create a classifier instance with sensible defaults.\n",
    "\n",
    "    Factory function that returns sklearn-compatible classifiers. Default\n",
    "    parameters are set for reasonable out-of-the-box performance, and can\n",
    "    be overridden by the params argument.\n",
    "\n",
    "    Args:\n",
    "        model_name: String identifier for the model. Supported values:\n",
    "            \"DummyMostFreq\", \"LogisticRegression\", \"RandomForest\", \"LightGBM\".\n",
    "        params: Optional dict of parameters to override defaults. Keys should\n",
    "            match the classifier's constructor arguments.\n",
    "\n",
    "    Returns:\n",
    "        A scikit-learn compatible classifier instance.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If model_name is not recognised.\n",
    "\n",
    "    Example:\n",
    "        >>> clf = create_base_estimator(\"LightGBM\", {\"n_estimators\": 200})\n",
    "        >>> clf.n_estimators\n",
    "        200\n",
    "    \"\"\"\n",
    "    params = dict(params or {})\n",
    "\n",
    "    if model_name == \"DummyMostFreq\":\n",
    "        return DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        base = {\n",
    "            \"max_iter\": 10000,\n",
    "            \"C\": 15,\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"n_jobs\": NUM_JOBS,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        }\n",
    "        base.update(params)\n",
    "        return LogisticRegression(**base)\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "        base = {\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\": None,\n",
    "            \"n_jobs\": NUM_JOBS,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        }\n",
    "        base.update(params)\n",
    "        return RandomForestClassifier(**base)\n",
    "\n",
    "    if model_name == \"LightGBM\":\n",
    "        base = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": -1,\n",
    "            \"num_leaves\": 31,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": 1,\n",
    "            \"verbose\": -1,\n",
    "        }\n",
    "        base.update(params)\n",
    "        return lgb.LGBMClassifier(**base)\n",
    "\n",
    "    raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "\n",
    "def suggest_params_for_model(trial, model_name):\n",
    "    \"\"\"Define Optuna search spaces for each model.\n",
    "\n",
    "    This function is called by Optuna during hyperparameter optimisation.\n",
    "    It uses the trial object to suggest parameter values, and Optuna\n",
    "    learns which regions of the parameter space yield better results.\n",
    "\n",
    "    Args:\n",
    "        trial: An Optuna trial object that provides suggest_* methods\n",
    "            for proposing hyperparameter values.\n",
    "        model_name: String identifier for the model being tuned.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A pair of (params_dict, sampling_strategy) where:\n",
    "            - params_dict: Dictionary of hyperparameter values to pass\n",
    "              to create_base_estimator().\n",
    "            - sampling_strategy: String (\"none\" or \"smote\") indicating\n",
    "              the resampling method to use.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If model_name has no defined search space.\n",
    "\n",
    "    Example:\n",
    "        >>> # Inside an Optuna objective function:\n",
    "        >>> params, sampling = suggest_params_for_model(trial, \"LightGBM\")\n",
    "        >>> estimator = create_base_estimator(\"LightGBM\", params=params)\n",
    "    \"\"\"\n",
    "    base_est = create_base_estimator(model_name)\n",
    "    base_params = base_est.get_params(deep=False)\n",
    "\n",
    "    if model_name == \"DummyMostFreq\":\n",
    "        return base_params, \"none\"\n",
    "\n",
    "    sampling_strategy = trial.suggest_categorical(\"sampling_strategy\", SAMPLING_METHODS)\n",
    "\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        params = dict(base_params)\n",
    "        params[\"C\"] = trial.suggest_float(\"C\", 1e-3, 1e3, log=True)\n",
    "        params[\"class_weight\"] = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = dict(base_params)\n",
    "        params[\"n_estimators\"] = trial.suggest_int(\"n_estimators\", 100, 300)\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "        params[\"min_samples_split\"] = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "        params[\"class_weight\"] = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    if model_name == \"LightGBM\":\n",
    "        params = dict(base_params)\n",
    "        params[\"n_estimators\"] = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "        params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-3, 3e-1, log=True)\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 50)\n",
    "        params[\"num_leaves\"] = trial.suggest_int(\"num_leaves\", 10, 100)\n",
    "        params[\"class_weight\"] = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "        return params, sampling_strategy\n",
    "\n",
    "    raise ValueError(f\"No search space for: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a249f62",
   "metadata": {},
   "source": [
    "## 9. Metrics Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90692a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)\n",
    "\n",
    "if TUNING_OBJECTIVE == \"mcc\":\n",
    "    tuning_scorer = mcc_scorer\n",
    "    tuning_scorer_name = \"MCC\"\n",
    "elif TUNING_OBJECTIVE == \"f1\":\n",
    "    tuning_scorer = make_scorer(f1_score, greater_is_better=True)\n",
    "    tuning_scorer_name = \"F1\"\n",
    "elif TUNING_OBJECTIVE == \"balanced_accuracy\":\n",
    "    tuning_scorer = make_scorer(balanced_accuracy_score, greater_is_better=True)\n",
    "    tuning_scorer_name = \"Balanced Accuracy\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown TUNING_OBJECTIVE: {TUNING_OBJECTIVE}\")\n",
    "\n",
    "print(f\"Optimizing for: {tuning_scorer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf704c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 10. Hyperparameter Tuning with Optuna\n",
    "\n",
    "**Optuna** uses Bayesian optimization (TPE) to efficiently search the hyperparameter space.\n",
    "This is more efficient than grid or random search for large search spaces.\n",
    "\n",
    "| Method | Strategy | Best For |\n",
    "|--------|----------|----------|\n",
    "| Grid Search | Exhaustive | Small spaces, few parameters |\n",
    "| Random Search | Random sampling | Initial exploration |\n",
    "| Bayesian (TPE) | Learns from trials | Large spaces, expensive evaluations |\n",
    "\n",
    "### Tuning Workflow\n",
    "\n",
    "For each model in `TUNED_MODELS`, we:\n",
    "1. Create an Optuna study to track optimisation progress\n",
    "2. Define an objective function that builds a pipeline and returns CV score\n",
    "3. Run `study.optimize()` which calls the objective N times\n",
    "4. Store the best parameters for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b842f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuning_cv():\n",
    "    \"\"\"Create cross-validation splitter for hyperparameter tuning.\n",
    "\n",
    "    Uses fewer repeats during tuning for speed, since we're comparing\n",
    "    many different hyperparameter configurations.\n",
    "\n",
    "    Returns:\n",
    "        RepeatedStratifiedKFold: CV splitter configured for tuning.\n",
    "    \"\"\"\n",
    "    return RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS_TUNING, random_state=SEED_TUNING)\n",
    "\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimisation.\n",
    "\n",
    "    This function is called by Optuna for each trial. It builds a pipeline\n",
    "    with the suggested hyperparameters, evaluates it via cross-validation,\n",
    "    and returns the mean score.\n",
    "\n",
    "    Args:\n",
    "        trial: Optuna trial object that provides suggest_* methods.\n",
    "        model_name: String identifier for the model being tuned.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean cross-validation score for the tuning metric.\n",
    "    \"\"\"\n",
    "    params, sampling_strategy = suggest_params_for_model(trial, model_name)\n",
    "    estimator = create_base_estimator(model_name, params=params)\n",
    "    pipe = build_pipeline(estimator, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    cv = make_tuning_cv()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=tuning_scorer, n_jobs=NUM_JOBS)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "tuning_records = []\n",
    "best_params_by_model = {}\n",
    "studies_by_model = {}\n",
    "\n",
    "for model_name in TUNED_MODELS:\n",
    "    print(f\"Starting Optuna tuning for: {model_name}\")\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler, study_name=f\"{model_name}_tuning\")\n",
    "\n",
    "    def _objective(trial):\n",
    "        return objective(trial, model_name)\n",
    "\n",
    "    N_TRIALS = N_TRIALS_PER_MODEL.get(model_name, 30)\n",
    "    study.optimize(_objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best {tuning_scorer_name} for {model_name}: {study.best_value:.4f}\")\n",
    "    print(\"Best params:\", study.best_params)\n",
    "\n",
    "    best_params_by_model[model_name] = study.best_params\n",
    "    studies_by_model[model_name] = study\n",
    "\n",
    "    for trial in study.trials:\n",
    "        rec = {\"model\": model_name, \"trial\": trial.number, f\"objective_{TUNING_OBJECTIVE}\": trial.value}\n",
    "        for k, v in trial.params.items():\n",
    "            rec[f\"param_{k}\"] = v\n",
    "        tuning_records.append(rec)\n",
    "\n",
    "tuning_results_df = pd.DataFrame(tuning_records) if tuning_records else pd.DataFrame()\n",
    "if not tuning_results_df.empty:\n",
    "    tuning_results_df.to_csv(os.path.join(tables_folder, \"optuna_tuning_all_trials.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37317910",
   "metadata": {},
   "source": [
    "For hyperparameter importance analysis and parallel coordinate visualizations,\n",
    "see `03_ce2_adult_census_income_pipeline_expert.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5299e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 11. Post-Tuning Cross-Validation\n",
    "\n",
    "After finding the best hyperparameters for each model, we perform a more\n",
    "rigorous cross-validation to get reliable performance estimates.\n",
    "\n",
    "**Why a separate CV phase?**\n",
    "- During tuning, we use a lightweight CV (fewer repeats) for speed\n",
    "- For final model comparison, we want more stable estimates\n",
    "- This also helps detect if we overfit to the tuning CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557782d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_cv():\n",
    "    \"\"\"Create cross-validation splitter for final model evaluation.\n",
    "\n",
    "    Uses more repeats than tuning CV for more stable performance estimates.\n",
    "\n",
    "    Returns:\n",
    "        RepeatedStratifiedKFold: CV splitter configured for evaluation.\n",
    "    \"\"\"\n",
    "    return RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS_CV, random_state=SEED_CV)\n",
    "\n",
    "\n",
    "cv_records = []\n",
    "rskf = make_eval_cv()\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    all_params = dict(best_params_by_model.get(model_name, {}))\n",
    "    sampling_strategy = all_params.pop(\"sampling_strategy\", \"none\")\n",
    "\n",
    "    estimator = create_base_estimator(model_name, params=all_params)\n",
    "    pipe = build_pipeline(estimator, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    print(f\"Evaluating model (CV): {model_name}\")\n",
    "    for split_idx, (train_idx, val_idx) in enumerate(rskf.split(X_train, y_train), start=1):\n",
    "        repeat_idx = (split_idx - 1) // N_SPLITS + 1\n",
    "        fold_idx = (split_idx - 1) % N_SPLITS + 1\n",
    "\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            y_val_pred = pipe.predict(X_val)\n",
    "\n",
    "        if hasattr(pipe, \"predict_proba\"):\n",
    "            y_val_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            y_val_proba = y_val_pred.astype(float)\n",
    "\n",
    "        mcc = matthews_corrcoef(y_val, y_val_pred)\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        prec = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "        rec = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "        bal_acc = balanced_accuracy_score(y_val, y_val_pred)\n",
    "        spec = specificity_score(y_val, y_val_pred)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        except ValueError:\n",
    "            roc_auc = np.nan\n",
    "\n",
    "        cv_records.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"repeat\": repeat_idx,\n",
    "                \"fold\": fold_idx,\n",
    "                \"sampling_strategy\": sampling_strategy,\n",
    "                \"mcc\": mcc,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"f1\": f1,\n",
    "                \"balanced_accuracy\": bal_acc,\n",
    "                \"specificity\": spec,\n",
    "                \"roc_auc\": roc_auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_records)\n",
    "cv_results_df.to_csv(os.path.join(tables_folder, \"cv_results_per_fold.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = [\"mcc\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"balanced_accuracy\", \"specificity\", \"roc_auc\"]\n",
    "model_comparison_df = pd.DataFrame()\n",
    "for metric in metric_cols:\n",
    "    model_comparison_df[f\"mean_{metric}\"] = cv_results_df.groupby(\"model\")[metric].mean()\n",
    "    model_comparison_df[f\"std_{metric}\"] = cv_results_df.groupby(\"model\")[metric].std()\n",
    "\n",
    "model_comparison_df = model_comparison_df.reset_index()\n",
    "model_comparison_df.to_csv(os.path.join(tables_folder, \"model_comparison_summary.csv\"), index=False)\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "model_comparison_df.sort_values(\"mean_mcc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4fb03",
   "metadata": {},
   "source": [
    "## 12. Select Best Model and Refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a18312",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_col = TUNING_OBJECTIVE\n",
    "mean_score_by_model = cv_results_df.groupby(\"model\")[objective_col].mean().sort_values(ascending=False)\n",
    "best_model_name = mean_score_by_model.index[0]\n",
    "print(f\"Best model by {TUNING_OBJECTIVE}: {best_model_name}\")\n",
    "\n",
    "best_params = best_params_by_model.get(best_model_name, {})\n",
    "all_params = best_params.copy()\n",
    "sampling_strategy = all_params.pop(\"sampling_strategy\", \"none\")\n",
    "\n",
    "best_estimator = create_base_estimator(best_model_name, params=all_params)\n",
    "best_pipeline = build_pipeline(best_estimator, sampling_strategy=sampling_strategy)\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "print(f\"Fitted best model pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6048f5",
   "metadata": {},
   "source": [
    "## 13. Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac12ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "y_test_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "test_rec = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "test_spec = specificity_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_pr_auc = average_precision_score(y_test, y_test_proba)\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test MCC: {test_mcc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test precision: {test_prec:.4f}\")\n",
    "print(f\"Test recall: {test_rec:.4f}\")\n",
    "print(f\"Test F1: {test_f1:.4f}\")\n",
    "print(f\"Test balanced accuracy: {test_bal_acc:.4f}\")\n",
    "print(f\"Test specificity: {test_spec:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lb.classes_)\n",
    "disp.plot(cmap=\"Reds\", values_format=\"d\")\n",
    "plt.gca().grid(False)\n",
    "plt.title(f\"Confusion matrix ({best_model_name})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_folder, f\"confusion_matrix{out_suffix}.pdf\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC = {test_roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC curve (test set)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_folder, f\"roc_curve{out_suffix}.pdf\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a02157",
   "metadata": {},
   "source": [
    "## 14. Threshold Optimisation\n",
    "\n",
    "Most classifiers output probabilities, and we convert these to class predictions\n",
    "using a **decision threshold** (default 0.5). However, the optimal threshold\n",
    "depends on:\n",
    "\n",
    "- **Class imbalance**: With imbalanced classes, 0.5 may not be the natural decision boundary learned by the model\n",
    "- **Cost structure**: If false negatives are more costly than false positives (or vice versa), we should adjust the threshold accordingly\n",
    "- **Metric of interest**: Different metrics are optimised at different thresholds\n",
    "\n",
    "Below we sweep through thresholds from 0 to 1 and compute each metric, then\n",
    "identify the optimal threshold for our chosen objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62061251",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.0, 1.0, 100)  # generate 100 thresholds from 0.0 to 1.0\n",
    "th_records = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_thr = (y_test_proba >= thr).astype(int)\n",
    "    th_records.append(\n",
    "        {\n",
    "            \"threshold\": thr,\n",
    "            \"mcc\": matthews_corrcoef(y_test, y_thr),\n",
    "            \"f1\": f1_score(y_test, y_thr, zero_division=0),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_test, y_thr),\n",
    "        }\n",
    "    )\n",
    "\n",
    "threshold_df = pd.DataFrame(th_records)\n",
    "threshold_df.to_csv(os.path.join(tables_folder, \"threshold_analysis.csv\"), index=False)\n",
    "\n",
    "# Find optimal thresholds\n",
    "optimal_thresholds = {}\n",
    "for metric in [\"mcc\", \"f1\", \"balanced_accuracy\"]:\n",
    "    optimal_idx = threshold_df[metric].idxmax()\n",
    "    optimal_thresholds[metric] = threshold_df.loc[optimal_idx, \"threshold\"]\n",
    "    print(f\"Optimal threshold for {metric}: {optimal_thresholds[metric]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimal threshold\n",
    "if USE_OPTIMAL_THRESHOLD:\n",
    "    optimal_threshold = optimal_thresholds.get(TUNING_OBJECTIVE, 0.5)\n",
    "    y_test_pred_opt = (y_test_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "    print(f\"\\nUsing optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"MCC (default 0.5): {test_mcc:.4f}\")\n",
    "    print(f\"MCC (optimal {optimal_threshold:.2f}): {matthews_corrcoef(y_test, y_test_pred_opt):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472caefa",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "This advanced tutorial covered:\n",
    "- ML pipelines with preprocessing and SMOTE resampling\n",
    "- Multiple classification metrics for imbalanced data\n",
    "- Hyperparameter tuning with Optuna (Bayesian optimization)\n",
    "- Threshold optimisation for different objectives\n",
    "\n",
    "**For additional topics**, see `03_ce2_adult_census_income_pipeline_expert.py`:\n",
    "- Ensemble methods (voting, stacking)\n",
    "- Probability calibration analysis\n",
    "- Feature importance analysis\n",
    "- Learning curves for overfitting diagnosis\n",
    "- Cost-benefit analysis\n",
    "- Additional sampling methods (ADASYN, undersampling, hybrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3.14.2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
